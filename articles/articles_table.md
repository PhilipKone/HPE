# Human Pose Estimation Research Articles

| Authors                                                                                                                                               | Title                                                                                                                      | Journal Name                                                                                        | Publishing House                                                                 | Year                               | Volume                  | Issue                    | Problem Solved                                                                                                                                                                                                                                                               | Method                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | Data & Availability                                                                                                                                                                                                                                                                                                              | Performance Metrics                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | Future Works                                                                                                                                                                                                                                 | Limitations                                                                                                                                                                                                                                                                                                                                                                                                                                           | Critique                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
| ----------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------- | ---------------------------------- | ----------------------- | ------------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Yanping Li, Ruyi Liu, Xiangyang Wang, Rui Wang                                                                                                        | Human pose estimation based on lightweight basic block                                                                     | Machine Vision and Applications                                                                     | Springer-Verlag GmbH Germany (part of Springer Nature)                           | 2023                               | 34                      | 3                        | Enable efficient deployment of human pose estimation models on devices with limited computing power by reducing the network's size and computational cost                                                                                                                    | Proposes a new module called "LiteBlock" that replaces the standard residual block in HRNet. LiteBlock uses depthwise separable convolutions and an inverted (reverse) bottleneck structure while reducing redundant activation and normalization layers to lower parameters and computation                                                                                                                                                                                                                                                                                       | COCO (COCO2017 with ~57K training, 5K validation, 20K test images, along with detailed JSON annotations) and MPII (with approx. 22,246 training and 11,731 test samples, annotated with keypoints)                                                                                                                               | On COCO validation: parameters drop from ca. 28.5M to 14.5M and GFLOPs from 7.1 to 2.9 with an AP of 73.9 (only a 0.5 point drop compared with the original model). On MPII, the average keypoint detection accuracy is around 89.83 (versus 90.33 for the original HRNet)                                                                                                                                                                                                                   | The authors point toward further application of the lightweight human pose estimation model in practical tasks such as pose tracking and human disorder diagnosis                                                                            | Slight performance trade-offs appear: for example, ablation studies showed that unnecessary extra activation/normalization can lead to a minor drop in precision (e.g., AP decrease on COCO validation)                                                                                                                                                                                                                                               | The approach effectively reduces model complexity and computation while keeping accuracy near state-of-the-art, though further tuning may be needed to balance efficiency and precision in diverse edge scenarios                                                                                                                                                                                                                                                                                                                              |
| Yu Cheng, Yihao Ai, Bo Wang, Xinchao Wang, Robby T. Tan                                                                                               | Bottom-up 2D pose estimation via dual anatomical centers for small-scale persons                                           | Pattern Recognition                                                                                 | Elsevier Ltd.                                                                    | 2023                               | 139                     | N/A                      | Improve the accuracy of multi-person 2D pose estimation—particularly for small-scale persons—by resolving the issues created by arbitrary bounding box centers and lack of human-scale normalization                                                                       | The paper proposes a two-pronged approach: (1) multi-scale training to better handle scale variations in persons with a single forward pass at test time, and (2) the use of dual anatomical centers (head and body centers) instead of arbitrary bounding box centers, supplemented by modules for pose similarity, pose confidence, and pose refinement                                                                                                                                                                                                                          | Evaluated on publicly available benchmark datasets such as COCO, OCHuman, and CrowdPose                                                                                                                                                                                                                                          | 71.0 AP on the COCO test-dev set (with single-scale testing)                                                                                                                                                                                                                                                                                                                                                                                                                                 | Not explicitly stated                                                                                                                                                                                                                        | Multi-scale training complexity and longer training times                                                                                                                                                                                                                                                                                                                                                                                             | Significant improvements for small-scale detection but added complexity                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| Han Yu, Congju Du, Li Yu                                                                                                                              | Scale-aware heatmap representation for human pose estimation                                                               | Pattern Recognition Letters                                                                         | Elsevier B.V.                                                                    | 2022                               | 154                     | N/A                      | Tackles the adverse effects of scale variation in heatmap construction for multi-person pose estimation and addresses the detection imbalance between keypoints                                                                                                              | Proposes a novel "Scale-Aware Heatmap Generator (SAHG)" that customizes the Gaussian kernel variance based on the relative scale of each keypoint along with a weight-redistributed loss function to prioritize harder samples during training                                                                                                                                                                                                                                                                                                                                     | Evaluated on publicly available data (MS-COCO 2017 with its train, val, and test-dev splits). MPII dataset is also mentioned for qualitative evaluation                                                                                                                                                                          | For single-scale testing, the method achieves about 67.5% AP on COCO val2017, while multi-scale testing on COCO test-dev reaches about 69.4% AP. Additional metrics such as AP50, AP75, AP for medium-scale (AP M) and large-scale (AP L) are detailed in the paper's Table 1                                                                                                                                                                                                                | Not explicitly mentioned                                                                                                                                                                                                                     | The method yields the strongest gains for large-scale persons but is less effective for small-scale bodies because using a common basic variance may exacerbate interference between nearby keypoints; it also relies on careful tuning of thresholds (e.g., JNT and scale threshold)                                                                                                                                                                 | While the approach demonstrates clear improvements over the baseline and competitive performance overall, its reliance on parameter tuning and limited gains for small-scale keypoints suggest room for further refinement to better handle all scale variations                                                                                                                                                                                                                                                                               |
| Ke Sun, Zigang Geng, Depu Meng, Bin Xiao, Dong Liu, Zhaoxiang Zhang, Jingdong Wang                                                                    | Bottom-Up Human Pose Estimation by Ranking Heatmap-Guided Adaptive Keypoint Estimates                                      | IEEE Transactions on Pattern Analysis and Machine Intelligence (submitted)                          | IEEE                                                                             | 2020                               | N/A                     | N/A                      | Enhancing the bottom-up human pose estimation pipeline by improving both the keypoint detection and grouping stages, while handling scale and orientation variance                                                                                                           | A heatmap-guided pixel-wise keypoint regression framework that integrates: (1) An adaptive representation transformation using a pixel-wise spatial transformer network (STN), (2) A joint shape and heatvalue scoring scheme for pose ranking, (3) A tradeoff heatmap estimation loss to counter the imbalance between keypoint and background pixels                                                                                                                                                                                                                             | Evaluated on publicly available benchmarks: COCO keypoint detection dataset and CrowdPose benchmark. Code is available on GitHub (HRNet-Bottom-up-Pose-Estimation)                                                                                                                                                               | Reported AP scores include: 70.2 (AP on COCO test-dev set for single-scale testing), 66.2 (AP on CrowdPose test set). Additional metrics such as AP50, AP75, AR, etc., are detailed in the paper                                                                                                                                                                                                                                                                                             | Not explicitly mentioned                                                                                                                                                                                                                     | Not directly detailed; however, the approach's reliance on high-resolution inputs and its multi-branch architecture may imply higher computational demands                                                                                                                                                                                                                                                                                            | The proposed method is architecturally complex with several integrated components that demand careful tuning, and this complexity may lead to increased computational overhead compared to more straightforward alternatives                                                                                                                                                                                                                                                                                                                   |
| Yongfeng Qi, Hengrui Zhang, Shengcong Wen, Anye Liang, Panpan Cao, Huili Chen                                                                         | Adaptive Module and Accurate Heatmap Translator for Multi-Person Human Pose Estimation                                     | Computers & Graphics                                                                                | Elsevier Ltd.                                                                    | 2024                               | 120                     | N/A                      | Addresses challenges in localizing human keypoints in multi-person images – especially for small human bodies – by reducing errors caused by coarse heatmap-to-image coordinate transformations                                                                            | Integrates three novel modules: (1) Adaptive Human Body Size Module (AHBZM): Uses trainable parameters to adapt multi-scale feature fusion according to body size, (2) Spatial Selective Attention Module (SSAM): Employs a dual-input/dual-output attention mechanism to enhance feature fusion, (3) More Accurate Heatmap Translator (MAHT): Computes sub-pixel offsets for precise coordinate mapping                                                                                                                                                                           | Uses established public benchmarks – COCO, MPII, and PoseTrack – with detailed experimental setups; the code is publicly available on GitHub                                                                                                                                                                                   | Reports improvements in average precision (AP) on COCO (e.g., a 1.9% boost with CPN and a 0.5% boost with HRNet-w48); additionally, competitive PCK/PCKh scores on MPII are shown                                                                                                                                                                                                                                                                                                            | Potential optimization of computational cost and enhanced integration with tracking algorithms                                                                                                                                               | The method introduces extra computational overhead (due to per-keypoint optimization in heatmap translation) and shows relatively modest gains in some metrics; the tracking performance (MOTA score) is also not the primary focus                                                                                                                                                                                                                   | Although the paper effectively addresses previously overlooked quantization issues and improves keypoint localization, the incremental performance improvements and increased complexity raise questions about scalability and the isolated impact of each added module                                                                                                                                                                                                                                                                        |
| Hua Li, Shiping Wen, Kaibo Shi                                                                                                                        | A simple and effective multi-person pose estimation model for low power embedded system                                    | Microprocessors and Microsystems                                                                    | Elsevier B.V.                                                                    | 2023                               | 96                      | N/A                      | Addresses the challenge of multi-person pose estimation on low-power embedded devices, particularly handling occluded and invisible keypoints                                                                                                                                | Proposes the Simple and Effective Network (SEN) that builds on a top‐down approach and enhances a simple baseline by integrating three novel modules: Feature Fusion Module (FFM), Channel Enhancement Attention Module (CEAM), and Feature Enhancement Module (FEM)                                                                                                                                                                                                                                                                                                              | Utilizes two well-known datasets – the COCO keypoints detection dataset and the MPII Human Pose dataset; training is carried out solely on standard datasets without additional data augmentation                                                                                                                               | COCO (for example, using ResNet-50 with 256×192 input: AP = 71.6; with 384×288 input: AP = 73.5) and MPII (best model achieves PCKh@0.5 = 90.0)                                                                                                                                                                                                                                                                                                                                            | Not explicitly mentioned                                                                                                                                                                                                                     | Not explicitly mentioned                                                                                                                                                                                                                                                                                                                                                                                                                              | Not explicitly mentioned                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
| Sen Yang, Ze Feng, Zhicheng Wang, Yanjie Li, Shoukui Zhang, Zhibin Quan, Shu-tao Xia, Wankou Yang                                                     | Detecting and grouping keypoints for multi-person pose estimation using instance-aware attention                           | Pattern Recognition                                                                                 | Elsevier Ltd.                                                                    | 2023                               | 136                     | N/A                      | Improves the grouping of detected keypoints for multi-person pose estimation without relying on handcrafted associative signals                                                                                                                                              | A transformer-based scheme that leverages the inherent self-attention mechanism and further supervises it using instance masks. The network combines a ResNet backbone with a Transformer encoder and introduces a novel attention-based parsing algorithm together with an auxiliary mask loss                                                                                                                                                                                                                                                                                    | Evaluated on publicly available datasets – the COCO keypoint detection challenge and COCO person instance segmentation dataset (with additional use of an ImageNet-pretrained ResNet)                                                                                                                                           | Object Keypoint Similarity (OKS) and mean Average Precision (AP) over multiple thresholds. The method shows comparable performance to CNN-based bottom-up approaches and, after refinement with a single-pose estimator, achieves about a 7 AP gain                                                                                                                                                                                                                                          | Potential refinement of self-attention supervision strategy and improving grouping algorithm                                                                                                                                                 | The method is sensitive to threshold selection in attention maps; self-attention's quadratic complexity can lead to redundancy; challenges handling jitter, missing, and inversion errors in keypoint localization                                                                                                                                                                                                                                    | Although the approach innovatively eliminates the need for human-defined association cues, it relies on empirical tuning and may suffer in highly challenging scenarios due to computational inefficiency and limited global assignment guarantees                                                                                                                                                                                                                                                                                             |
| Longsheng Wei, Xuefu Yu, Zhiheng Liu                                                                                                                  | Human Pose Estimation in Crowded Scenes using Keypoint Likelihood Variance Reduction                                       | Displays                                                                                            | Elsevier B.V.                                                                    | 2024                               | 83                      | N/A                      | Improving human pose estimation in crowded scenes by overcoming challenges related to noisy keypoint detection and isolated body parts caused by occlusion                                                                                                                   | Proposes a novel decoding method by introducing the Keypoint Likelihood Variance Reduction (KLVR) algorithm to set dynamic thresholds for keypoint heatmaps; implements a noise keypoint filter to remove spurious peaks; incorporates a Cycle Skeleton Structure (CSS) to reconnect isolated parts; and employs an Identity Mapping Hourglass Network (IMHN) architecture for inference                                                                                                                                                                                           | Experiments are conducted on publicly available datasets – the CrowdPose dataset (20,000 images with 80,000 human instances, annotated with 14 keypoints) and the COCO dataset (over 200,000 images with 250,000 person instances annotated with 17 keypoints)                                                                  | The approach is evaluated using OKS-based metrics such as AP50, AP75, mAP, and mAR. Outperforms previous approaches on the CrowdPose test dataset                                                                                                                                                                                                                                                                                                                                            | Not explicitly mentioned                                                                                                                                                                                                                     | The method's performance may be sensitive to the pre-set thresholds and hyperparameters (e.g., a fixed decision boundary of 0.1 is mentioned), and the optimization process could add computational overhead in highly crowded scenes                                                                                                                                                                                                                 | While the KLVR method and subsequent noise filtering and CSS add valuable improvements for crowded scenes, further work is needed to assess real-time applicability, the robustness of the hyperparameter settings across diverse scenarios, and to better quantify performance gains with detailed numerical analysis                                                                                                                                                                                                                         |
| Ziwen Chen, Yu Cai, Qihua Wang, Lijie Cao                                                                                                             | POLO: Pose Estimation with One-Stage Location-Sensitive Coding                                                             | Computers and Electronics in Agriculture                                                            | Elsevier                                                                         | 2024                               | 226                     | N/A                      | Addresses the need for precise and real-time fish pose estimation in aquaculture to monitor key fish behaviors (feeding, health, movement) and overcome the limitations of traditional two-stage methods                                                                     | Introduces a novel single-stage multi-object pose estimation framework called POLO that directly predicts fish keypoints by partitioning the image into grids and using location-sensitive coding. Features include adaptive Gaussian kernel generation, ConvNeXt backbone with FPN, and a hybrid loss function                                                                                                                                                                                                                                                                    | A custom, meticulously annotated fish keypoint dataset comprising over 3,000 images with more than 26,000 instances (each with six keypoints). Dataset availability not explicitly stated                                                                                                                                        | 65.34% OKS Average Precision (AP); 71.4 FPS on Tesla V100                                                                                                                                                                                                                                                                                                                                                                                                                                    | Potential extension by adjusting output channels to adapt to other datasets and integration into edge computing devices                                                                                                                      | Study focuses on fish pose estimation; performance on occluded or highly variable scales and simple decoder design may limit broader applicability                                                                                                                                                                                                                                                                                                    | The framework is innovative and achieves impressive real-time performance on a custom dataset, but reliance on domain-specific dataset and relatively simple decoder design might restrict generalizability                                                                                                                                                                                                                                                                                                                                    |
| Zhaokun Li, Qiong Liu                                                                                                                                 | CRENet: Crowd region enhancement network for multi-person 3D pose estimation                                               | Image and Vision Computing                                                                          | Elsevier B.V.                                                                    | 2024                               | 151                     | N/A                      | Tackles the challenge of recovering accurate multi-person absolute 3D poses from a single image, specifically addressing depth ambiguities and errors in root depth regression                                                                                               | Proposes a novel bottom-up framework, CRENet, integrating two key modules: (1) Feature Decoupling Module (FDM) to separately learn discriminative features for absolute and relative depths, and (2) Global Attention Module (GAM) to focus on informative crowd regions while suppressing irrelevant areas                                                                                                                                                                                                                                                                        | Evaluated on public benchmark datasets such as MuPoTS-3D and CMU Panoptic                                                                                                                                                                                                                                                        | Demonstrates superiority over state-of-the-art bottom-up methods in absolute 3D pose estimation                                                                                                                                                                                                                                                                                                                                                                                              | Not explicitly mentioned                                                                                                                                                                                                                     | Requires two-stage training strategy due to risk of unstable gradient flow; attention modules may sometimes focus on non-informative regions affecting prediction accuracy                                                                                                                                                                                                                                                                            | While promising improved accuracy in absolute multi-person 3D pose estimation, the approach introduces additional complexity and potential training instability. Detailed quantitative breakdown of performance gains not presented                                                                                                                                                                                                                                                                                                            |
| Rabeea Jaffari, M.A. Hashmani, Constantino Carlos Reyes-Aldasoro, A.Z. Junejo, H. Taib, M. Nasir Abdullah                                             | PLPose: An efficient framework for detecting power lines via key points-based pose estimation                              | Journal of King Saud University – Computer and Information Sciences                                | Elsevier B.V. on behalf of King Saud University                                  | 2023                               | 35                      | N/A                      | Effective detection of electrical power lines in aerial images for UAV-based inspection—addressing the challenges of thin structures and cluttered backgrounds                                                                                                              | Recasts power line detection as a top-down, key point-based pose estimation task. Modifies vanilla MobileNetV3 (termed kMobileNetV3) by appending a deconvolutional key point head to predict three representative key points per power line, and integrates an Unbiased Data Processing (UDP) module                                                                                                                                                                                                                                                                              | Three benchmark datasets: Mendeley PL Dataset (publicly available), PLDU (urban scenes), and PLDM (mountain scenes). Code available on GitHub                                                                                                                                                                                    | Processing speed of 29 FPS and model size of 5.23 million parameters, with competitive precision and recall compared to state-of-the-art methods                                                                                                                                                                                                                                                                                                                                             | Potential extensions for powergrid fault detection mentioned in Section 6                                                                                                                                                                    | Study confined to three-keypoint annotation scheme and relies on relatively small benchmark datasets                                                                                                                                                                                                                                                                                                                                                  | Although efficient and lightweight, further validation on larger and more diverse datasets—and comparison with transformer-based line detection—needed to establish generalizability and robustness                                                                                                                                                                                                                                                                                                                                          |
| Muyu Li, Yingfeng Wang, Henan Hu, Xudong Zhao                                                                                                         | InferTrans: Hierarchical structural fusion transformer for crowded human pose estimation                                   | Information Fusion                                                                                  | Elsevier B.V.                                                                    | 2025                               | 117                     | N/A                      | Tackles the unique challenges of multi-person pose estimation in crowded scenes, especially handling frequent occlusions and intertwined human poses                                                                                                                         | A Transformer-based architecture named "InferTrans" that deploys a hierarchical joint-limb-semantic fusion module to integrate global structural and local semantic features                                                                                                                                                                                                                                                                                                                                                                                                       | Reports extensive experiments on multiple datasets (specific datasets not explicitly provided)                                                                                                                                                                                                                                   | Claims to outperform existing methods in challenging, crowded scenarios (explicit numerical metrics not provided)                                                                                                                                                                                                                                                                                                                                                                            | Not specified                                                                                                                                                                                                                                | Not explicitly listed; mainly contrasts improvements over known limitations of GCN-based approaches                                                                                                                                                                                                                                                                                                                                                   | No explicit critique provided in the available content                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
| Xiaoyan Jiang, Hangyu Tao, Jenq-Neng Hwang, Zhijun Fang                                                                                               | A Multiscale Coarse-to-Fine Human Pose Estimation Network With Hard Keypoint Mining                                        | IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS: SYSTEMS                                         | IEEE                                                                             | 2024                               | 54                      | 3                        | Tackles the challenge of accurately estimating "hard" keypoints (i.e., occluded, small, and ambiguous keypoints) in multiperson pose estimation                                                                                                                              | Introduces the HM2PN framework, a two-stage (coarse-to-fine) network: (1) CoarseNet uses novel multiscale fusion module to detect "simple" keypoints, (2) FineNet refines detections and handles "hard" keypoints with dynamic hard keypoint mining loss integrated with spatial and channel-wise attention                                                                                                                                                                                                                                                                        | Evaluated on the COCO keypoint benchmark. Source code available on GitHub                                                                                                                                                                                                                                                        | Reports superior performance compared to state-of-the-art methods on the COCO benchmark (exact metric values not specified)                                                                                                                                                                                                                                                                                                                                                                  | Not explicitly mentioned                                                                                                                                                                                                                     | Potential sensitivity regarding hyperparameter tuning (e.g., choice of scaling factor in hard keypoint mining loss)                                                                                                                                                                                                                                                                                                                                   | Not explicitly mentioned; could benefit from discussion of alternative architectures or real-time implications                                                                                                                                                                                                                                                                                                                                                                                                                                 |
| Jiahua Wu, Hyo-Jong Lee                                                                                                                               | A New Multi-Person Pose Estimation Method Using the Partitioned CenterPose Network                                         | Appl. Sci.                                                                                          | MDPI, Basel, Switzerland                                                         | 2021                               | 11                      | N/A                      | Tackles the challenge of grouping detected joint candidates into correct person instances in bottom-up multi-person pose estimation                                                                                                                                          | Introduces the Partitioned CenterPose (PCP) Network that incorporates a novel Partition Pose Representation (PPR) to integrate person instance data with joint offsets, and employs an improved L1 loss to refine offset prediction                                                                                                                                                                                                                                                                                                                                                | Uses the MS COCO and CrowdPose datasets, which are publicly available                                                                                                                                                                                                                                                            | With DLA-34 backbone: AP = 0.634, AP0.5 = 0.864, AP0.75 = 0.693, APM = 0.575, APL = 0.739, AR = 0.698, inference time of 0.039 sec. AP values range between 0.634 and 0.668 across implementations                                                                                                                                                                                                                                                                                           | Not explicitly mentioned                                                                                                                                                                                                                     | Relies on clear visibility of part centers; in cases of occlusion, uses estimated "illusion centers" which may affect encoding accuracy. Shows slightly inferior performance compared to methods with more powerful backbones or larger input sizes                                                                                                                                                                                                   | Offers excellent balance between speed and accuracy, achieving competitive results among bottom-up approaches. However, gains in AP are incremental and dependence on accurately detecting body part centers may challenge robustness in complex or occluded scenes                                                                                                                                                                                                                                                                            |
| Tianyu Shen, Deqi Li, Fei-Yue Wang, Hua Huang                                                                                                         | Depth-Aware Multi-Person 3D Pose Estimation With Multi-Scale Waterfall Representations                                     | IEEE Transactions on Multimedia                                                                     | IEEE                                                                             | 2023                               | 25                      | Not specified            | Accurate estimation of absolute 3D poses for multiple persons from a single monocular image, addressing challenges of occlusions and scale variations                                                                                                                        | A two-branch framework: a 2D branch employs a waterfall-based multi-scale representation with Gaussian heatmap modulation and a Fusion-Net to integrate local and global features; a 3D branch (DA-Net) leverages joint heatmaps, part affinity fields (PAFs), and joint depth maps for depth-aware 3D pose regression and joint grouping                                                                                                                                                                                                                                          | Uses established benchmarks – MuCo-3DHP, MuPoTS-3D – and introduces an occluded MuPoTS-3D dataset                                                                                                                                                                                                                              | Evaluated via 3DPCK (both relative and absolute) metrics with comprehensive quantitative and qualitative analysis showing improvements over state-of-the-art methods (e.g., SMAP); exact numerical values are not detailed in the snippet                                                                                                                                                                                                                                                    | Section V (Conclusion) hints at further research directions, such as improved occlusion handling and enhanced depth estimation; however, specific future work details aren't elaborated in the provided content                              | May struggle in extreme cases where bounding boxes of persons overlap completely, potentially affecting the reliability of joint grouping                                                                                                                                                                                                                                                                                                             | While the framework shows significant promise by integrating depth cues and multi-scale features, its reliance on accurate depth estimation and potential computational complexity suggest opportunities for further optimization and testing in real-world, unconstrained scenarios                                                                                                                                                                                                                                                           |
| Yi Jiang, Kexin Yang, Jinlin Zhu, LiQin                                                                                                               | YOLO-Rlepose: Improved YOLO Based on Swin Transformer and Rle-Oks Loss for Multi-Person Pose Estimation                    | Electronics                                                                                         | MDPI, Basel, Switzerland                                                         | 2024                               | 13                      | Not specified            | Tackles challenges in multi-person 2D pose estimation—such as occlusion, noise, and non-rigid body movements                                                                                                                                                                | A direct regression-based method featuring a YOLO-Rlepose network that enhances the standard C3 module by integrating a Swin Transformer branch (C3STR) along with a novel Rle-Oks loss                                                                                                                                                                                                                                                                                                                                                                                            | Experiments conducted on the COCO dataset (a publicly available benchmark)                                                                                                                                                                                                                                                       | Achieved an AP of 65.01 on COCO, outperforming YOLO-Pose by 2.11% in AP                                                                                                                                                                                                                                                                                                                                                                                                                      | Not specified                                                                                                                                                                                                                                | Not explicitly discussed                                                                                                                                                                                                                                                                                                                                                                                                                              | Not provided explicitly; potential increased complexity from integrating multiple advanced modules could be discussed further                                                                                                                                                                                                                                                                                                                                                                                                                  |
| Feng Wang, Gang Wang, Baoli Lu                                                                                                                        | YOLOv8-PoseBoost: Advancements in Multimodal Robot Pose Keypoint Detection                                                 | Electronics                                                                                         | MDPI, Basel, Switzerland                                                         | 2024                               | 13                      | N/A                      | Addresses the difficulties in motion keypoint detection for multimodal robots, particularly the challenges of small target detection and accurate perception in complex scenes                                                                                               | Proposes YOLOv8-PoseBoost, which integrates several innovations: a Channel Attention Module (CBAM) to increase sensitivity on small targets, multiple scale detection heads, cross-level connectivity channels for enhanced feature fusion, and the SIoU loss for improved training convergence and accuracy                                                                                                                                                                                                                                                                       | Utilizes well-known public datasets—the COCO Dataset and the MPII Human Pose Dataset—with the article published under an open access license                                                                                                                                                                                   | The paper demonstrates improved detection accuracy and faster convergence, although specific numerical metrics were not provided in the excerpt                                                                                                                                                                                                                                                                                                                                              | Future research is expected to focus on further improvements in accuracy, enhanced robustness, and extendibility of the method to a broader range of multimodal robotic applications                                                         | No explicit limitations of the proposed method are detailed, although the work acknowledges ongoing challenges in highly dense and dynamic scenarios                                                                                                                                                                                                                                                                                                  | The document does not provide an explicit critique; however, one might consider evaluating the computational overhead and real-world adaptability in further studies                                                                                                                                                                                                                                                                                                                                                                           |
| XianFeng Tang, Shuwei Zhao                                                                                                                            | The application prospects of robot pose estimation technology: exploring new directions based on YOLOv ApexNet             | Frontiers in Neurorobotics                                                                          | Frontiers Media                                                                  | Not specified (Published in April) | Not provided            | Not provided             | It targets the shortcomings in current human motion pose estimation methods applied to service robots – namely, low keypoint localization accuracy, limited robustness and real‐time performance                                                                           | An improved version of the YOLO framework – YOLOv8-ApexNet – is proposed. It integrates two novel components: Bidirectional Routing Attention (BRA) and a Generalized Feature Pyramid Network (GFPN) to enhance feature fusion and detection performance                                                                                                                                                                                                                                                                                                                         | Experiments employ the publicly available COCO and MPII Human Pose datasets along with self-collected sports activity video data                                                                                                                                                                                                 | The paper reports significant improvements in keypoint localization accuracy and robustness compared to existing methods; however, no detailed numerical metrics are provided                                                                                                                                                                                                                                                                                                                | Not explicitly mentioned                                                                                                                                                                                                                     | Not explicitly mentioned                                                                                                                                                                                                                                                                                                                                                                                                                              | Not explicitly mentioned                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
| Congju Du, Zengqiang Yan, Han Yu, Li Yu, and Zixiang Xiong                                                                                            | Hierarchical Associative Encoding and Decoding for Bottom-Up Human Pose Estimation                                         | IEEE Transactions on Circuits and Systems for Video Technology                                      | IEEE                                                                             | 2023                               | 33                      | 4                        | Addresses the challenge of reliably grouping detected keypoints to individual human instances in bottom-up pose estimation – overcoming the limitation of treating all keypoints equally and ignoring their inherent relationships                                          | Proposes a hierarchical associative framework that incorporates multi-level (keypoint, group, and instance) associations. The method uses an associative encoding module that leverages prior knowledge about human skeletons via focal pulling loss, alongside a progressive associative decoding strategy that dynamically groups keypoints based on updated reference embeddings                                                                                                                                                                                                | Evaluated on standard datasets (MS-COCO, CrowdPose, and MPII). Source code and pretrained models are available on GitHub at https://github.com/ducongju/HAE                                                                                                                                                                      | Superior performance is demonstrated on benchmark datasets. Specific numerical performance metrics are not directly provided in the excerpt                                                                                                                                                                                                                                                                                                                                                  | Not explicitly mentioned                                                                                                                                                                                                                     | Not explicitly mentioned                                                                                                                                                                                                                                                                                                                                                                                                                              | Not explicitly provided. Overall, while the framework is well-motivated and shows promising improvements, further discussion on component-wise contributions and potential constraints under varying scenarios would strengthen the work                                                                                                                                                                                                                                                                                                       |
| M. Y. Li, J. Zhao                                                                                                                                     | CE-HigherHRNet: Enhancing Channel Information for Small Persons Bottom-Up Human Pose Estimation                            | IAENG International Journal of Computer Science                                                     | Not mentioned (likely IAENG)                                                     | 2022                               | 49                      | 1                        | Overcomes the difficulty of detecting keypoints for small persons in crowded scenes caused by scale variations, channel information loss, and aliasing effects during cross-scale fusion                                                                                     | Proposes the CE-HigherHRNet architecture combining a multi-scale sub-pixel skip fusion module, a lightweight attention mechanism (with both channel and spatial attention modules), and a high-resolution feature pyramid that employs Dupsampling instead of deconvolution                                                                                                                                                                                                                                                                                                        | Evaluated on publicly available datasets (COCO test-dev & CrowdPose)                                                                                                                                                                                                                                                             | COCO test-dev: 71.9% AP (1.4% improvement over HigherHRNet); Small persons: 68.1% AP (1.5% improvement); CrowdPose: 66.6% AP as reported in experiments                                                                                                                                                                                                                                                                                                                                      | Not explicitly mentioned                                                                                                                                                                                                                     | Not explicitly discussed                                                                                                                                                                                                                                                                                                                                                                                                                              | Not explicitly provided or critiqued                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
| Shuhena Salam Aonty, Pranab Kumar Dhar, Kaushik Deb, Moumitasensarma, Tetsuya Shimamura                                                               | Multi-Person Pose Estimation Using Group-Based Convolutional Neural Network Model                                          | IEEE Access                                                                                         | IEEE                                                                             | 2023                               | 11                      | N/A                      | Addresses multi-person pose estimation challenges – specifically occlusion, complex backgrounds, and the difficulty of accurately localizing body keypoints in crowded images                                                                                               | Proposes a bottom-up CNN approach that extracts features via multiple branches. The method uses confidence maps for keypoint detection, cluster maps for grouping keypoints, affinity fields to associate parts, and an extended "occlusion net" branch to detect occluded/invisible keypoints                                                                                                                                                                                                                                                                                     | Experiments are conducted on publicly available datasets (e.g., the MPII dataset is mentioned for keypoint annotation)                                                                                                                                                                                                           | Reports a maximum accuracy of 93% for mean PCKh, surpassing several state-of-the-art methods                                                                                                                                                                                                                                                                                                                                                                                                 | The paper mentions future research directions regarding further refinement of the approach, though specific details are not fully elaborated in the provided content                                                                         | The work acknowledges challenges in accurately detecting, localizing, and grouping body parts in the presence of occlusion and complex scenes                                                                                                                                                                                                                                                                                                         | While the method achieves high accuracy, the paper would benefit from deeper discussion on computational efficiency, scalability, and the potential trade-off between complexity and real-time performance                                                                                                                                                                                                                                                                                                                                     |
| Jia Li, Meng Wang                                                                                                                                     | Multi-Person Pose Estimation With Accurate Heatmap Regression and Greedy Association                                       | IEEE Transactions on Circuits and Systems for Video Technology                                      | IEEE                                                                             | 2022                               | 32                      | 8                        | Localizing 2D human pose keypoints for multiple persons – with an emphasis on challenging scenarios like crowded scenes where overlapping and scale variations occur                                                                                                        | An advanced bottom‑up approach that: (1) Encodes multi‑person pose information using refined Gaussian heatmaps for keypoint localization; (2) Introduces peak regularization to enhance ranking of heatmap responses; (3) Uses a novel focal L2 loss to address imbalanced keypoint detection; (4) Implements guiding offset fields for greedy keypoint association; and (5) Upgrades the Hourglass‑104 network (dubbed Hourglass‑104MA) with multi‑scale heatmap aggregation                                                                                                 | Experimental evaluation is performed on public benchmarks – specifically the COCO and CrowdPose datasets, which are widely available in the research community                                                                                                                                                                  | COCO: 71.9% AP; CrowdPose: 70.5% AP                                                                                                                                                                                                                                                                                                                                                                                                                                                          | Not explicitly mentioned                                                                                                                                                                                                                     | The approach remains sensitive to small errors in keypoint localization (e.g., a few pixel shifts) and, in earlier representations, body part heatmaps were found less precise when dealing with overlapping or small poses                                                                                                                                                                                                                           | Although the method achieves state‑of‑the‑art performance by addressing limitations in conventional heatmap regression (notably through peak regularization and improved association), its reliance on specific interpolation methods (bicubic over bilinear) and a relatively complex network design may pose challenges in generalizing to other scenarios or improving computational efficiency                                                                                                                                          |
| Yalong Jiang, Wenrui Ding, Hongguang Li, Zheru Chi                                                                                                    | Multi-Person Pose Tracking With Sparse Key-Point Flow Estimation and Hierarchical Graph Distance Minimization              | IEEE Transactions on Image Processing                                                               | IEEE                                                                             | 2024                               | 33                      | Not specified            | Addresses the challenges in multi-person pose tracking by tackling issues arising from occlusions, motion blur, and incomplete body joint observations                                                                                                                       | Introduces a novel framework that combines a Sparse Key-Point Flow Estimating Module (SKFEM) and a Hierarchical Graph Distance Minimizing Module (HGMM), leveraging pixel-level appearance consistency, human-level structural consistency, and temporal propagation                                                                                                                                                                                                                                                                                                               | Evaluated on publicly available datasets, specifically the PoseTrack 2017, 2018, and 2021 datasets, and refers to benchmarks like the COCO Key-points Challenge                                                                                                                                                                  | Reported to outperform state-of-the-art methods on benchmarks; however, specific numerical metrics are not provided in the extracted content                                                                                                                                                                                                                                                                                                                                                 | Not explicitly mentioned                                                                                                                                                                                                                     | Potential limitations include handling extreme occlusions and missed detections in dynamic or crowded scenes, as well as unspecified computational overhead and real-time performance concerns                                                                                                                                                                                                                                                        | The approach is innovative in combining sparse key-point flow estimation and hierarchical graph modeling, but its complexity and reliance on historical frame propagation may challenge processing speed and generalization                                                                                                                                                                                                                                                                                                                    |
| Lei Jin, Xiaojuan Wang, Xuecheng Nie, Shuicheng Yan, Wendong Wang, Jian Zhao, Yandong Guo                                                             | Rethinking the Person Localization for Single-Stage Multi-Person Pose Estimation                                           | IEEE Transactions on Multimedia                                                                     | IEEE                                                                             | 2024                               | 26                      | Not specified            | Addresses the suboptimal performance in single-stage multi-person pose estimation caused by the independent handling of person localization and body structure perception                                                                                                    | Proposes the Structure-guided Person Localization (SPL) framework, which integrates two novel components: Structure-guided Center Learning (SCL) and Agency-based Scale-adaptive Learning (ASL) to jointly optimize the center and displacement maps in an end-to-end manner                                                                                                                                                                                                                                                                                                       | Evaluated on standard benchmarks including the COCO keypoint detection dataset (train2017, val2017, test2017) and CrowdPose. Source codes, pretrained models, and online demos are planned to be released upon acceptance                                                                                                        | Achieves 72.1 AP on COCO test-dev2017 and 69.5 AP on CrowdPose; additional results include 70.2 AP (HRNet-W32) and 72.8 AP (HRNet-W48 with 640-size input) on COCO val2017                                                                                                                                                                                                                                                                                                                   | Future release of source code, pretrained models, and online demos; potential refinements in further optimizing the joint learning approach for pose estimation                                                                              | Not explicitly stated in the provided content                                                                                                                                                                                                                                                                                                                                                                                                         | No explicit critique provided; the paper focuses on showing improved performance over existing methods without incurring extra inference-time overhead                                                                                                                                                                                                                                                                                                                                                                                         |
| Lipeng Ke, Ming-Ching Chang, Honggang Qi, Siwei Lyu                                                                                                   | DetPoseNet: Improving Multi-Person Pose Estimation via Coarse-Pose Filtering                                               | IEEE TRANSACTIONS ON IMAGE PROCESSING                                                               | IEEE                                                                             | 2022                               | 31                      | N/A                      | Tackles the early commitment issues in person detection (especially in crowded and occluded scenes) and the high computational cost caused by redundant proposals in multi-person pose estimation                                                                            | An end-to-end multi-person detection and pose estimation framework that consists of a three-stage network: (1) a lightweight coarse-pose proposal extraction sub-net to simultaneously generate human detection bounding boxes and body keypoint proposals, (2) a coarse-pose based proposal filtering module that refines detections by leveraging pose information to eliminate false positives and recover false negatives, and (3) a multi-scale pose refinement sub-net employing multi-scale supervision, multi-scale regression, structure-aware loss, and keypoint masking | Evaluated using publicly available datasets – COCO and OCHuman                                                                                                                                                                                                                                                                  | Reported a 5–6× computational speedup and an 80% reduction in system complexity; evaluation metrics include the COCO Object-Keypoint Similarity (OKS) among others (exact numerical AP values are not provided)                                                                                                                                                                                                                                                                            | Not explicitly mentioned                                                                                                                                                                                                                     | Not explicitly mentioned                                                                                                                                                                                                                                                                                                                                                                                                                              | Not explicitly mentioned                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
| Xiangyang Wang, Tao Pei, Rui Wang                                                                                                                     | Enhanced keypoint information and pose-weighted re-ID features for multi-person pose estimation and tracking               | Machine Vision and Applications                                                                     | Springer-Verlag GmbH Germany, part of Springer Nature                            | 2024                               | 35                      | Not specified            | Tackles challenges in multi-person pose estimation and tracking in videos – notably issues due to pose occlusion, motion blur, and unreliable re-ID matching                                                                                                                | Proposes a three-pronged method: (1) a Decouple Heatmap Network (DHN) to extract enhanced keypoint confidence and position; (2) a Pose Construction Module (PCM) to reconstruct occluded poses using spatio-temporal relationships; and (3) a Pose Embedding Network (PEN) to weight re-ID features with a matching strategy based on the Hungarian algorithm                                                                                                                                                                                                                      | Uses popular benchmark datasets – PoseTrack 2017 and PoseTrack 2018, with additional training on COCO2017. Source code is publicly available on GitHub (https://github.com/TaoTaoPei/posetracking)                                                                                                                              | For pose estimation, improvements of 1.6–2.2% mAP over baseline methods on PoseTrack 2017 and 0.7–0.9% mAP gains on PoseTrack 2018 validation. For tracking, a MOTA of 69.0% is achieved on the PoseTrack 2018 validation set                                                                                                                                                                                                                                                              | Not explicitly mentioned                                                                                                                                                                                                                     | Not explicitly discussed; although one competitor (TKMRNet) shows higher mAP on one dataset, the proposed method highlights its efficiency through a smaller parameter count                                                                                                                                                                                                                                                                          | No explicit critique provided within the paper; overall, the method is competitive with noted trade-offs between efficiency and absolute performance                                                                                                                                                                                                                                                                                                                                                                                           |
| Chengang Dong, Guodong Du                                                                                                                             | An enhanced real‐time human pose estimation method based on modified YOLOv8 framework                                     | Scientific Reports                                                                                  | Nature Publishing Group (inferred)                                               | 2024                               | 14                      | 8012 (article ID) or N/A | Tackles the accuracy loss in real‑time human pose estimation caused by partial occlusion and limited receptive field in conventional YOLOv8‑based methods                                                                                                                  | Proposes the CCAM-Person model which modifies the YOLOv8x‑pose baseline by integrating three main improvements: a Multi‑scale Receptive Field (MRF) module to reduce feature loss, a Multi‑path Feature Pyramid Network (MFPN) for enhanced cross‑level feature fusion, and a Context Coordinate Attention Module (CCAM) to improve keypoint regression                                                                                                                                                                                                                        | Validated on open‑source datasets – specifically MS COCO 2017 and CrowdPose                                                                                                                                                                                                                                                    | Achieves an average precision improvement of 2.8% on MS COCO 2017 and 3.5% on CrowdPose compared to the baseline YOLOv8x‑pose                                                                                                                                                                                                                                                                                                                                                               | While no detailed "future works" section is provided, the discussion suggests further optimizations may target handling dense crowds, severe occlusions, and reducing computational overhead for real‑time applications                     | The method, while addressing occlusion and receptive field issues, may still encounter challenges in very dense scenarios and could increase computational complexity compared to the baseline                                                                                                                                                                                                                                                        | The enhancements yield clear precision gains; however, the additional modules (MRF, MFPN, and CCAM) might lead to increased model complexity and potential trade-offs in inference speed, especially on resource‑constrained devices                                                                                                                                                                                                                                                                                                          |
| Haixin Wang, Lu Zhou, Yingying Chen, Ming Tang, Jinqiao Wang                                                                                          | Semi-Supervised Learning for Detector-Free Multi-Person Pose Estimation                                                    | Machine Intelligence Research                                                                       | Springer-Verlag GmbH Germany, part of Springer Nature                            | 2024                               | N/A                     | N/A                      | Addresses the challenge of achieving accurate multi-person human pose estimation when labeled data are scarce. In particular, it tackles the difficulty of mining instance-aware information in unlabeled images in detector-free (bottom-up and single-stage) frameworks    | Proposes an end-to-end semi-supervised training strategy that combines consistency regularization on heatmaps with a pseudo-labeling approach based on keypoint clustering. A greedy matching algorithm is introduced to improve the generation of reliable instance-specific pseudo labels                                                                                                                                                                                                                                                                                        | Uses publicly available benchmarks – COCO Keypoint (with splits: ~64K Train, 5K Val, 20K Test‑dev) and CrowdPose (with splits: ~10K Train, 2K Val, 8K Test) – to evaluate the method                                                                                                                                          | Evaluated with Average Precision (AP) computed from Object Keypoint Similarity (OKS) over multiple thresholds. For example, when applied to a baseline (Associative Embedding – AE), the method improves performance by approximately +11.9 AP on CrowdPose (at 5% labeling) and +14.7 AP on COCO Keypoint (at 1% labeling), among other improvements                                                                                                                                       | Not explicitly discussed in the provided excerpt                                                                                                                                                                                             | The approach may be sensitive to the quality of pseudo labels and relies heavily on the effectiveness of the keypoint clustering. In early training stages, global matching strategies (e.g., Hungarian algorithm) produced suboptimal pseudo labels, necessitating the greedy matching workaround                                                                                                                                                    | The work shows promising improvements in low-label regimes; however, further discussion is needed on how it would handle highly complex or occluded scenes. Moreover, a deeper analysis of the method's robustness across different architectures and real-world conditions would strengthen the contribution                                                                                                                                                                                                                                  |
| Bruno Artacho, Andreas Savakis                                                                                                                        | Full-BAPose: Bottom Up Framework for Full Body Pose Estimation                                                             | Sensors                                                                                             | MDPI, Basel, Switzerland                                                         | 2023                               | 23                      | N/A                      | Provides accurate, full-body pose estimation—including body, hands, feet, and facial landmarks—in multi-person images. It overcomes challenges posed by occlusions, scale variations, and reliance on external person detectors                                            | A bottom-up, end-to-end deep learning framework that employs an encoder-decoder architecture. It leverages an HRNet backbone along with a Disentangled Waterfall Atrous Spatial Pyramid (D-WASP) module that uses multi-scale representations and adaptive convolutions for precise keypoint regression                                                                                                                                                                                                                                                                            | Public, well-established datasets: COCO, CrowdPose, and COCO-WholeBody (which provide extensive labeled keypoint data)                                                                                                                                                                                                           | Reported state-of-the-art (SOTA) results include: an Average Precision (AP) of 72.2% on the CrowdPose dataset and 68.4% on COCO-WholeBody, along with improvements in AP75 and overall error reductions compared to previous methods                                                                                                                                                                                                                                                         | Plans to extend the framework with vision transformers to further boost performance                                                                                                                                                          | No explicit limitation is mentioned; however, as with many deep architectures, there is an implied trade-off between accuracy and computational cost—especially when using higher-resolution variants                                                                                                                                                                                                                                                | The paper does not offer a self-critical discussion; overall, the approach is presented as a robust improvement over previous SOTA methods                                                                                                                                                                                                                                                                                                                                                                                                     |
| Jianqiang Zhang, Jing Hou, Qiusheng He, Zhengwei Yuan, and Hao Xue                                                                                    | MambaPose: A HumanPoseEstimation Based on Gated Feedforward Network and Mamba                                              | Sensors                                                                                             | MDPI, Basel, Switzerland                                                         | 2024                               | 24                      | Not specified / N/A      | The work addresses challenges in multi-person human pose estimation, particularly the false and missed detections in dense crowds and the difficulty in detecting small targets                                                                                              | The paper proposes a novel GMamba structure that integrates a gated feedforward network with slice downsampling and multi-channel feature fusion. In addition, an adaptive threshold focus loss (ATFL) is employed to assign higher weights to error-prone keypoints, thereby enhancing the model's detection precision even under occlusion and complex background conditions                                                                                                                                                                                                     | Tested on the publicly available COCO 2017 validation set                                                                                                                                                                                                                                                                        | Average Precision (AP): 72.2; AP50: 92.6 (with a 1.1% improvement on AP50 compared to typical methods)                                                                                                                                                                                                                                                                                                                                                                                       | Not specified in the provided excerpt                                                                                                                                                                                                        | Not mentioned                                                                                                                                                                                                                                                                                                                                                                                                                                         | No explicit critique provided                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
| Prasun Roy, Saumik Bhattacharya, Subhankar Ghosh, Umapada Pal                                                                                         | Multi-scale Attention Guided Pose Transfer                                                                                 | Pattern Recognition                                                                                 | Elsevier Ltd.                                                                    | 2023                               | 137                     | Not specified            | Generating realistic images of a person in a novel pose, while preserving detailed appearance (e.g., facial features, dress texture) across pose variations                                                                                                                  | A multi-scale attention guided network architecture comprising dual encoders (one for the condition image and one for pose heatmaps) and an upsampling decoder with attention links at every resolution. The model is trained adversarially (using a PatchGAN discriminator) and employs L1 loss combined with perceptual loss (via VGG19) to enhance visual fidelity                                                                                                                                                                                                              | Uses the DeepFashion dataset (with 176×256 person images) and the Market-1501 dataset (with 128×128 person images). Code implementation and pre-trained models are available publicly on GitHub (github.com/prasunroy/pose-transfer)                                                                                           | DeepFashion: SSIM = 0.769, IS = 3.379, DS = 0.976, PCKh = 0.98, LPIPS = 0.111 `<br>`Market-1501: SSIM = 0.266, IS = 3.682, DS = 0.74, PCKh = 0.95 (compared with state‐of‐the‐art methods)                                                                                                                                                                                                                                                                                              | The study mentions potential extension of the architecture to related tasks (e.g. semantic reconstruction, virtual try-on, skeleton-guided style manipulation) and indicates that further refinement of evaluation metrics may be beneficial | While not explicitly detailed as a "limitation", the paper discusses how upsampling from lower resolutions has historically led to a loss of fine details—an issue which the proposed approach seeks to mitigate but may still be a challenge. Additionally, current perceptual metrics may not fully capture human visual quality                                                                                                                   | An anomaly noted in the evaluation shows that one variant (A1-LR) can score slightly better in some metrics (IS and DS) despite generating visually inferior images. This highlights the limitations of existing evaluation metrics in generalizing human perception and suggests the need for more robust measures                                                                                                                                                                                                                            |
| Chuchu Han, Xin Yu, Changxin Gao, Nong Sang, Yi Yang                                                                                                  | Single Image Based 3D Human Pose Estimation via Uncertainty Learning                                                       | Pattern Recognition                                                                                 | Elsevier Ltd.                                                                    | 2022                               | 132                     | N/A                      | Tackles the inherent ambiguity in monocular 3D human pose estimation caused by depth information loss and occlusions, which otherwise leads to overfitting and poor generalization                                                                                           | Proposes an uncertainty-based framework that: (1) Models each body joint as a Laplace distribution (capturing both its estimated position and uncertainty), (2) Introduces an adaptive uncertainty-aware scaling factor within the soft-argmax operation to facilitate faster convergence and robustness, (3) Incorporates an uncertainty-aware graph convolutional network (UAGCN) to refine joint estimates by leveraging the relationships among joints and their uncertainties                                                                                                 | Uses established datasets: Human3.6M for single-person 3D pose estimation, MuCo-3DHP and MuPoTS-3D for multi-person scenarios, COCO data for augmentation and 2D keypoint training                                                                                                                                               | Evaluated primarily via mean per joint position error (MPJPE). On the Human3.6M dataset, the MPJPE improves from a baseline of ~72.83 mm to 66.76 mm after applying uncertainty learning and UAGCN. Other metrics include PA-MPJPE, 3DPCK, and AUC                                                                                                                                                                                                                                           | Not explicitly mentioned in the provided excerpt                                                                                                                                                                                             | The paper does not detail explicit limitations, though one may note potential sensitivity to hyperparameter choices (e.g., the range settings of the adaptive scaling factor) and the additional computational complexity incurred by the uncertainty estimation components                                                                                                                                                                           | The framework is innovative in jointly learning both 3D joint positions and their uncertainties, yielding measurable accuracy gains. However, its reliance on careful hyperparameter tuning and increased system complexity might challenge deployment in more unconstrained, real‑world scenarios. Further evaluation on diverse, in‑the‑wild datasets would strengthen its practical applicability                                                                                                                                        |
| Boshen Zhang, Yang Xiao, Fu Xiong, Cunlin Wu, Zhiguo Cao, Ping Liu, Joey Tianyi Zhou                                                                  | 3D human pose estimation with cross-modality training and multi-scale local refinement                                     | Applied Soft Computing                                                                              | Elsevier B.V.                                                                    | 2022                               | 122                     | Not specified            | Tackles challenges in 3D human pose estimation using depth data by addressing the domain shift between RGB and depth modalities and by refining coarse predictions using localized information                                                                               | Proposes a cross-modality CNN training strategy that adapts a pre-trained RGB model (ResNet-50) to depth data—using a partial Batch Normalization strategy and the addition of a normal vector map to the depth input—combined with a coarse-to-fine, multi-scale local refinement network                                                                                                                                                                                                                                                                                       | Uses well-established, publicly-available depth datasets: ITOP and K2HPD; additionally, a subset of the NTU RGB-D dataset is employed to analyze feature distribution (domain shift) between RGB and depth data                                                                                                                  | Evaluation is carried out using metrics such as Percent of Detected Joints (PDJ), Percentage of Correct Keypoints (PCKh@0.5), and mAP under the 10 cm-rule. The multi-scale refinement stage is reported to enhance accuracy by about 2.5% over the coarse stage                                                                                                                                                                                                                             | Not explicitly discussed in the provided content; however, there is room for exploring optimal local region scaling and assessing the approach on more diverse or noisy datasets                                                             | The method's effectiveness depends on the proper selection of the local region scale, and the limited size of depth datasets can still pose overfitting risks alongside persistent challenges due to imaging noise and domain differences                                                                                                                                                                                                             | The approach makes an innovative use of transfer learning from RGB to depth; however, its sensitivity to hyperparameter tuning (e.g., local scale selection) and the lack of detailed discussion on computational efficiency or generalizability across more varied scenarios are areas that could be further strengthened                                                                                                                                                                                                                     |
| Congju Du, LiYu                                                                                                                                       | A scale-sensitive heatmap representation for multi-person pose estimation                                                  | IET Image Processing                                                                                | John Wiley & Sons Ltd on behalf of The Institution of Engineering and Technology | 2022                               | 16                      | 4                        | It tackles the inaccuracy of heatmap representations in multi-person pose estimation caused by keypoint scale variation                                                                                                                                                      | Proposes a scale-sensitive heatmap algorithm that first computes a relative scale for each person instance (via a keypoint-based method) and then generates adaptive heatmaps by (1) adjusting the standard deviation of Gaussian kernels, (2) setting a limited (truncated) area to restrict background influence, and (3) modifying the kernel shape (shape-aware heatmaps) for elliptical joints                                                                                                                                                                                | Experiments were performed on public datasets – Microsoft COCO and CrowdPose. Additionally, the code and pretrained models are available on GitHub (https://github.com/ducongju/Scale-sensitive-Heatmap)                                                                                                                        | Achieves 70.0 AP with multi-scale testing and 68.4 AP with single-scale testing on the COCO test-dev2017 set; it outperforms HRNetBU/SWAHR by +1.8/+0.5 AP                                                                                                                                                                                                                                                                                                                                   | Not specified                                                                                                                                                                                                                                | No explicit limitations are discussed. However, the paper notes that setting appropriate hyperparameters (e.g., the optimal standard deviation and truncated radius) is critical and may pose challenges                                                                                                                                                                                                                                              | Not provided                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
| Yanlei Gu, Huiyang Zhang, Shunsuke Kamijo                                                                                                             | Multi-Person Pose Estimation using an Orientation and Occlusion Aware Deep Learning Network                                | Sensors                                                                                             | MDPI                                                                             | 2020                               | 20                      | N/A                      | Improve the accuracy of multi-person pose estimation—especially under conditions of occlusion and ambiguous body orientation—by exploiting more holistic and discriminative cues                                                                                           | A deep learning multi-task framework built upon Mask R-CNN. The approach integrates parallel and serial multi-task networks that simultaneously predict body segmentation, joint heatmaps, body orientation, and occlusion conditions. It further augments the standard COCO keypoints dataset with additional ground truths for orientation and occlusion                                                                                                                                                                                                                         | Public COCO keypoints dataset augmented with extra annotations for body orientation and mutual‑occlusion (publicly available)                                                                                                                                                                                                   | Percentage of Correct Keypoints (PCK): 84.6%; Correct Detection Rate (CDR): 83.7%                                                                                                                                                                                                                                                                                                                                                                                                            | The paper hints at further optimizations of network configurations and additional surveys of related methods, though explicit future directions are not extensively detailed                                                                 | The method might depend heavily on the quality of the added annotations; handling severe occlusion and potential computational complexity remain challenges                                                                                                                                                                                                                                                                                           | The multi-task strategy is innovative and helps reduce over‑detection, yet further validation on diverse datasets and real‑world scalability tests would strengthen the claims                                                                                                                                                                                                                                                                                                                                                               |
| Congcong Li, Yifan Li, Bin Wang, Yuting Zhang                                                                                                         | Research into the Applications of a Multi-Scale Feature Fusion Model in the Recognition of Abnormal Human Behavior         | Sensors                                                                                             | MDPI, Basel, Switzerland                                                         | 2024                               | 24                      | Not specified            | Tackles the need for timely and accurate recognition of abnormal human behaviors (e.g., falling, twitching, dizziness, vomiting) among the elderly, overcoming poor generalization and single-action recognition issues in existing computer vision models                   | Proposed a novel MSCS-DenseNet-LSTM model that integrates a multi-scale convolution (MSCS) module into DenseNet, replaces standard convolution blocks with an improved Inception Dense structure, and incorporates a dual LSTM network augmented with a CBAM attention mechanism for enhanced spatiotemporal feature fusion. Custom RGB (RIDS) and contour (CIDS) image datasets were also built to address dataset limitations                                                                                                                                                    | Two custom data collections were assembled: an RGB image dataset (RIDS) and a contour image dataset (CIDS), both derived from simulated abnormal behavior experiments using a Kinect 2.0 camera with 25 participants in a controlled environment                                                                                 | The model achieved accuracy, sensitivity, and specificity of approximately 98.80%, 98.75%, and 98.82% on one dataset, and 98.30%, 98.28%, and 98.38% on the other dataset, respectively                                                                                                                                                                                                                                                                                                      | The paper does not explicitly outline future work; however, it implies that further validation in real-world and uncontrolled scenarios, as well as additional model optimization, may be beneficial                                         | Although the authors address known issues in prior DenseNet-LSTM frameworks (such as insufficient early feature extraction and high parameter counts), the study does not discuss limitations regarding the reliance on simulated data and controlled experimental conditions, which may affect generalization in real-world applications                                                                                                             | While the multi-scale fusion approach significantly improves recognition performance, more discussion on model complexity, broader applicability, and scalability is warranted. Additional evaluation on diverse and real-world datasets would further strengthen the work                                                                                                                                                                                                                                                                     |
| Yaoping Li, Shuangcheng Jia, Qian Li                                                                                                                  | BalanceHRNet: An effective network for bottom-up human pose estimation                                                     | Neural Networks                                                                                     | Elsevier Ltd.                                                                    | 2023                               | 161                     | Not specified            | Addresses the drop in performance when estimating human poses in crowded or high overlap scenarios by overcoming limited receptive field and weak multi-scale representation                                                                                                 | Proposes a novel bottom-up approach—BalanceHRNet—that uses a balanced high-resolution module (BHRM) to connect nearly all convolutional layers for an enlarged receptive field and a branch attention module (BAM) to fuse multi-resolution features                                                                                                                                                                                                                                                                                                                             | Tested on publicly available datasets: CrowdPose and the COCO (2017) keypoint detection dataset                                                                                                                                                                                                                                  | For example, BalanceHRNet (at input size 640) achieved an AP of 63.0% (improving by 3.1% over HigherHRNet and by 1.6% over HigherHRNet-w32) along with additional metrics, including AP.5, AP.75, AP(E), AP(M) and AP(H) as detailed in Table 2 of the article                                                                                                                                                                                                                               | Future work may focus on enhancing the detection of dense and small-sized objects and overall efficiency in multi-scale identification                                                                                                       | The model shows poorer performance in identifying dense and small-scale objects, which can be a significant drawback in highly crowded scenes                                                                                                                                                                                                                                                                                                         | While the network outperforms several alternatives, its high computational cost (GFLOPs) and lack of pre-trained model utilization may limit its practical deployment compared to some top-down approaches that handle multi-scale cues more efficiently                                                                                                                                                                                                                                                                                       |
| Ke Sun, Bin Xiao, Dong Liu, Jingdong Wang                                                                                                             | Deep High-Resolution Representation Learning for Human Pose Estimation                                                     | arXiv preprint                                                                                      | arXiv                                                                            | 2019                               | N/A                     | N/A                      | Accurate human pose (keypoint) estimation while preserving high-resolution features throughout the process                                                                                                                                                                   | HRNet: A network that maintains high-resolution representations via parallel multi-resolution subnetworks and repeated multi-scale fusion instead of recovering them from a low-resolution bottleneck                                                                                                                                                                                                                                                                                                                                                                              | Training and evaluation are conducted on public benchmarks: COCO keypoint detection dataset, MPII Human Pose dataset, and PoseTrack; the code and models are publicly available at GitHub                                                                                                                                        | On COCO validation, HRNet‑W32 achieves an AP ~734 while HRNet‑W48 obtains higher AP values (approximately 751–755); on MPII, HRNet‑W32 reports a PCKh@0.5 of about 92.3; on PoseTrack, HRNet‑W48 reports mAP of 749 and MOTA of 579                                                                                                                                                                                                                                                     | Not explicitly mentioned in the paper                                                                                                                                                                                                        | Not explicitly discussed – aside from ablation studies regarding resolution and fusion details, any explicit limitations are not highlighted                                                                                                                                                                                                                                                                                                         | The article does not include a dedicated critique section; however, the ablation studies reveal sensitivity to input resolution and fusion design with potential trade‑offs in computation when increasing model capacity                                                                                                                                                                                                                                                                                                                     |
| Bowen Cheng, Bin Xiao, Jingdong Wang, Honghui Shi, Thomas S. Huang, Lei Zhang                                                                         | HigherHRNet: Scale-Aware Representation Learning for Bottom-Up Human Pose Estimation                                       | arXiv (preprint)                                                                                    | N/A (preprint repository)                                                        | 2020                               | N/A                     | N/A                      | Addresses the challenge of scale variation in bottom-up human pose estimation, with particular focus on accurately predicting keypoints for small persons                                                                                                                    | Proposes the HigherHRNet architecture that builds a high-resolution feature pyramid by incorporating one deconvolution module on top of HRNet. It uses multi-resolution supervision during training and a heatmap aggregation strategy during inference to generate scale-aware, high-resolution heatmaps                                                                                                                                                                                                                                                                          | Evaluated on the COCO Keypoint Detection dataset and CrowdPose; the authors provide the code and models publicly at https://github.com/HRNet/Higher-HRNet-Human-Pose-Estimation                                                                                                                                                  | For example, the HRNet-W48 variant of HigherHRNet (with multi-scale test) achieves: 70.5 AP, 89.3 AP50, and 77.2 AP75 on COCO test-dev. A notable gain is a reported improvement of about 25 AP for medium-scale persons over previous best bottom-up methods. Additionally, on CrowdPose, it surpasses top-down methods with an AP of 67.6                                                                                                                                                  | The authors mention exploring the optimal number of deconvolution modules—which appears dataset dependent—and further validation on additional datasets                                                                                    | Cascading too many deconvolution modules may cause performance to drop (due to misalignment between feature map and object scales), and the improvement is mainly pronounced for medium-scale persons rather than large persons                                                                                                                                                                                                                       | While the method substantially boosts performance on smaller scales, its increased architectural complexity and the sensitivity of performance gains to deconvolution design invite further scrutiny and tuning for broader applications                                                                                                                                                                                                                                                                                                       |
| Ke Sun, Zigang Geng, Depu Meng, Bin Xiao, Dong Liu, Zhaoxiang Zhang, Jingdong Wang                                                                    | Bottom-Up Human Pose Estimation by Ranking Heatmap-Guided Adaptive Keypoint Estimates                                      | IEEE Transactions on Pattern Analysis and Machine Intelligence (submitted; hence "to be published") | IEEE                                                                             | 2020                               | N/A (Not specified)     | N/A (Not specified)      | Addresses challenges in bottom-up human pose estimation—specifically improving the accuracy of keypoint localization and grouping for multiple persons. It deals with issues such as unknown numbers of persons, diverse scales and orientations, and confusing backgrounds | A novel framework that combines: (1) Heatmap-guided pixel-wise keypoint regression: Uses the estimated keypoint heatmaps to guide more accurate keypoint offset prediction. (2) Adaptive Representation Transformation: Applies a pixel‑wise spatial transformer network to learn adaptive representations that handle local scale and orientation variance. (3) Joint Shape and Heatvalue Scoring: Employs a small network to rank candidate poses by combining heatmap confidence with spatial (shape) features                                                                 | Evaluated on publicly available datasets—including COCO (train2017, val2017, test-dev2017) and CrowdPose—with code available on GitHub (HRNet-Bottom-up-Pose-Estimation)                                                                                                                                                       | Achieves state-of-the-art performance; for instance, using HrHRNet-W48 as the backbone obtains an Average Precision (AP) of approximately 70.2 on the COCO test-dev set. Metrics such as AP, AP50, AP75, and AR show notable improvements over comparable methods                                                                                                                                                                                                                            | The paper does not explicitly outline future work but hints at potential directions such as further optimization for computational efficiency and exploring additional ways to integrate scale/rotation invariance more seamlessly           | The reliance on high-resolution representations and multiple specialized modules (e.g., adaptive transformation units and joint scoring) may increase computational cost, potentially affecting real-time performance or requiring more extensive tuning                                                                                                                                                                                              | The approach is innovative with strong quantitative results on challenging benchmarks. However, its multi-stage design and reliance on several hyper-parameters add training complexity and may present trade-offs between accuracy and computational efficiency in practical deployment                                                                                                                                                                                                                                                       |
| Zigang Geng, Chunyu Wang, Yixuan Wei, Ze Liu, Houqiang Li, Han Hu                                                                                     | HumanPose as Compositional Tokens                                                                                          | arXiv Preprint                                                                                      | arXiv (self-published preprint)                                                  | 2023                               | N/A                     | N/A                      | Tackles the issue that conventional human pose estimation treats joints independently, which can lead to unrealistic predictions under occlusion                                                                                                                             | Presents a structured representation of pose as discrete compositional tokens (PCT). A compositional encoder converts an input pose into multiple token features; these are quantized via a shared codebook, and a decoder reconstructs the pose, while pose estimation is reformulated as a classification task over token categories                                                                                                                                                                                                                                             | Evaluated on public benchmarks such as COCO, MPII, CrowdPose, OCHuman, SyncOCC (for 2D occlusion cases), and Human3.6M (for 3D poses). The code and models are publicly available on GitHub (https://github.com/Gengzigang/PCT)                                                                                                  | Competitive results with state-of-the-art performance: on COCO test sets, AP values range roughly from 76.5 to 78.3 (depending on model size), MPII PCKh scores around 92–93, and lower MPJPE for 3D pose estimation; notably, it shows marked improvements on occluded joints (e.g., AP Occluded values upward of 45.6 in some evaluations)                                                                                                                                                | Future work includes further reducing ambiguities by incorporating environmental context (e.g. using cues from surrounding objects) under the discrete representation framework                                                              | The method still exhibits some failure cases in severe occlusions and relies on fixed backbone features and discrete token quantization, which may require careful tuning (such as the number of tokens and codebook size) to balance performance and classification complexity                                                                                                                                                                       | While showing promising improvements especially in occlusion scenarios, the approach depends on quantization and token discretization choices that could limit flexibility. End-to-end training or further integration of contextual cues may be needed to overcome remaining challenges                                                                                                                                                                                                                                                       |
| Hung‑Cuong Nguyen, Thi‑Hao Nguyen, Jakub Nowak, Aleksander Byrski, Agnieszka Siwocha, Van‑Hung Le                                                  | Combined YOLOv5 and HRNet for High Accuracy 2D Keypoint and Human Pose Estimation                                          | JAISCR                                                                                              | Not specified                                                                    | 2022                               | 12                      | 4                        | Automates human detection and 2D pose estimation directly on full images—overcoming the limitation of prior methods that relied on cropped images                                                                                                                           | A novel combination of a pre‑trained YOLOv5 (augmented with Contextual Constraints for robust human detection) and HRNet for accurate 2D keypoint/pose estimation; manual marking of bounding boxes on the Human 3.6M dataset supports evaluation                                                                                                                                                                                                                                                                                                                                 | Human 3.6M dataset: a benchmark indoor dataset with approximately 3.6 million images and detailed 2D/3D human pose annotations (publicly available)                                                                                                                                                                              | Processing speed: 55 FPS (Protocol #1); Mean error distance: 5.14 pixels (on 1000×1000 images); Accuracy: 94.8% PCK and 99.2% PDJ@0.4 (head joint)                                                                                                                                                                                                                                                                                                                                          | The work foreshadows extending the system to full 3D human pose estimation and 3D point cloud generation                                                                                                                                     | The approach may be prone to false detections in cluttered scenes (as illustrated by misidentifying a wall camera) and relies on the assumption that the largest bounding box corresponds to the human. It also depends on manual annotations for evaluation, which could affect scalability                                                                                                                                                          | While the combination of YOLOv5 and HRNet achieves impressive speed and accuracy on a well-known dataset, the dependency on contextual constraints and manual bounding box selection might limit its robustness, especially in highly crowded or occluded real‑world scenarios. Further comparative evaluations in diverse conditions would strengthen the assessment                                                                                                                                                                         |
| Zhengxiong Luo, Zhicheng Wang, Yan Huang, Liang Wang, Tieniu Tan, Erjin Zhou                                                                          | Rethinking the Heatmap Regression for Bottom-up Human Pose Estimation                                                      | arXiv (cs.CV)                                                                                       | arXiv (Preprint)                                                                 | 2021                               | N/A                     | N/A                      | It addresses the limitation of using fixed standard deviations in Gaussian-based heatmap regression for bottom-up human pose estimation. In particular, it tackles challenges posed by large variations in human scales and inherent labeling ambiguities                    | Introduces a two-fold solution: (1) Scale-Adaptive Heatmap Regression (SAHR) that adaptively adjusts the Gaussian kernel's standard deviation for each keypoint, and (2) Weight-Adaptive Heatmap Regression (WAHR) to rebalance the loss between fore- and background samples. When used together, they are referred to as SWAHR                                                                                                                                                                                                                                                   | Primarily evaluated on the COCO dataset (train2017, val2017, test-dev2017) and additionally on the CrowdPose dataset. The authors also publish source code (GitHub link) for replication                                                                                                                                         | The proposed approach shows a significant improvement – for instance, achieving a ~+15AP boost and a 720AP score on COCO test-dev2017. Detailed metrics (AP, AP50, AP75, APM, APL) are provided in the experimental tables                                                                                                                                                                                                                                                                  | No explicit future work is mentioned                                                                                                                                                                                                         | The SAHR method may aggravate the imbalance between foreground and background samples and appears sensitive to the setting of hyperparameters                                                                                                                                                                                                                                                                                                         | While the method is innovative and demonstrates promising improvements in accuracy, further evaluation across additional datasets and an in‑depth analysis of computational overhead and hyperparameter sensitivity would strengthen its broader applicability                                                                                                                                                                                                                                                                                |
| Soonchan Park, Jinah Park                                                                                                                             | Position Puzzle Network and Augmentation: localizing human keypoints beyond the bounding box                               | Machine Vision and Applications                                                                     | Springer-Verlag GmbH Germany (Springer Nature)                                   | 2023                               | 34                      | Not specified            | Addresses the limitation of conventional human pose estimation that only considers keypoints within a bounding box. It tackles the challenge of localizing human keypoints outside of the pre-defined bounding box in cases where the human object is only partially visible | Proposes a dual approach: (1) Position Puzzle Network (PPNet) which refines the bounding box to include missing keypoint regions, and (2) Position Puzzle Augmentation (PPAugmentation) which augments training by constructing cropped sub-regions (via a fixed cropping policy) together with a tailored loss (combining Generalized IoU and Puzzle Size Loss)                                                                                                                                                                                                                   | Utilizes the widely used COCO dataset—with its Training, Validation, and Test splits—to construct both a conventional dataset and an additional "Crop-COCO" test set. The cropping method is applied to generate partial human images simulating occluded views                                                                | In evaluations on the Crop-COCO test set, the method improves the baseline keypoint detectors by an average of 39.5% in mean average precision (mAP) and 30.5% in mean average recall (mAR). Detailed analyses across varying cropping scenarios (easy, moderate, hard) show improvements that, in some cases, reach up to more than 100% enhancement in mAP or mAR under challenging conditions                                                                                             | No explicit future work is outlined; however, supplementary sections indicate that further in-depth analysis of the Puzzle Size Loss and additional refinements may be pursued                                                               | The method presents challenges in extreme cropping conditions. In particular, the refinement may not fully capture certain keypoints (e.g., knees and ankles) when only a very limited part of the human object is visible                                                                                                                                                                                                                            | The approach is innovative in extending keypoint detection beyond the conventional bounding box. While the integration of a dedicated network and augmentation strategy is promising, the reliance on fixed cropping policies and the inherent difficulty of accurately predicting occluded keypoints suggest that further validation and potential adjustments are needed for robust performance in diverse, real-world scenarios                                                                                                             |
| Hei Law, Jia Deng                                                                                                                                     | CornerNet: Detecting Objects as Paired Keypoints                                                                           | ECCV (2018)                                                                                         | ECCV Proceedings (Springer LNCS)*                                                | 2018                               | N/A                     | N/A                      | Addresses limitations of one-stage detectors that rely on thousands of anchor boxes—specifically the imbalance between positive and negative samples and the many hyperparameters required                                                                                  | They reformulate object detection as detecting a pair of keypoints (the top-left and bottom-right corners of a bounding box) using a single convolutional network. The network produces two heatmaps (one for each corner), learns an embedding for grouping paired corners (inspired by associative embedding), and applies a novel "corner pooling" layer to better localize the corners. An hourglass network is used as the backbone to capture both global and local features                                                                                                 | Uses the MS COCO dataset for detection experiments. Source code is publicly available on GitHub (https://github.com/umichvl/CornerNet)                                                                                                                                                                                           | Achieves a 42.1% average precision (AP) on MS COCO test-dev. Ablation studies show, for example, that the inclusion of corner pooling improves AP by around +2.0% (with additional metrics such as AP50 and AP75 reported)                                                                                                                                                                                                                                                                   | Not explicitly discussed                                                                                                                                                                                                                     | Its performance is highly dependent on accurate detection of corners; errors in predicted corner heatmaps (and hence in paired keypoint grouping) are identified as the main performance bottleneck                                                                                                                                                                                                                                                   | The approach is highly novel and removes the need for elaborate anchor box designs; however, its reliance on precise corner detection may limit its robustness, particularly for objects with less well-defined corners or in crowded scenes, and the method's sensitivity to the grouping process could be further improved                                                                                                                                                                                                                   |
| Aouaidjia Kamel, Bin Sheng, Ping Li, Jinman Kim, David Dagan Feng                                                                                     | Hybrid Refinement-Correction Heatmaps for Human Pose Estimation                                                            | IEEE Transactions on Multimedia                                                                     | IEEE                                                                             | 2021                               | 23                      | Not specified            | Addressing the challenges in accurately estimating human pose in images—including handling occlusion, variation in scene/background, complex body poses, and erroneous joint localization                                                                                   | A hybrid deep learning approach that employs two convolutional neural network models: a Refinement Network (RNet) based on uniform hourglass architectures and a Correction Network (CNet) with hourglasses of varying depths. The fusion of detection heatmaps (via element-wise multiplication and addition) refines joint location predictions                                                                                                                                                                                                                                  | Utilizes publicly available benchmark datasets, notably the MPII Human Pose Dataset (≈25K images with over 40K annotated people) and the FLIC dataset                                                                                                                                                                           | Evaluated using PCK and PCKh (at a 0.5 threshold) metrics on the MPII validation set, with detection curves demonstrating that the fusion of RNet and CNet yields competitive and sometimes superior performance compared to existing single-scale methods (specific numerical values are not detailed)                                                                                                                                                                                      | The work suggests extending the approach to deeper, more fused models with additional hourglasses and further investigating applications to multi-person pose estimation                                                                     | Failure cases occur in extreme situations—for instance, when body parts are severely occluded or when the subject is extremely close to the camera; training challenges such as unstable loss behavior (which required tuning of learning rates) are also noted                                                                                                                                                                                      | The novel dual-network strategy offers an innovative solution for more robust pose estimation by combining complementary feature extractions. However, it also leads to increased computational complexity and has so far been primarily validated on single-person scenarios, with multi-person evaluations remaining largely qualitative                                                                                                                                                                                                     |
| Jia Li, Meng Wang                                                                                                                                     | Multi-Person Pose Estimation With Accurate Heatmap Regression and Greedy Association                                       | IEEE Transactions on Circuits and Systems for Video Technology                                      | IEEE                                                                             | 2022                               | 32                      | 8                        | Tackles the challenging problem of multi-person pose estimation by localizing all 2D human keypoints in images—even in crowded scenes with occlusions and scale variations                                                                                                  | Proposes an advanced bottom-up approach that employs an encoding-decoding scheme using Gaussian heatmaps combined with guiding offset fields to perform greedy keypoint association. The method improves upon the conventional Hourglass architecture (upgraded to Hourglass-104MA with multi-scale heatmap aggregation) and introduces a novel focal L2 loss to better address the imbalanced nature of keypoint detection in heatmap regression                                                                                                                                  | Evaluations are carried out on public benchmark datasets, notably the COCO and CrowdPose datasets                                                                                                                                                                                                                                | Achieves state-of-the-art results with an Average Precision (AP) of 71.9% on the COCO dataset and 70.5% AP on the CrowdPose dataset                                                                                                                                                                                                                                                                                                                                                          | The paper does not explicitly outline future work; however, the discussion hints at potential refinement in handling occlusions and developing more robust keypoint ranking techniques                                                       | Some issues remain with the heatmap interpolation process: if neighboring response values are too similar, the true keypoints might not be clearly distinguished. Additionally, earlier attempts (e.g., body part heatmaps) were found to be insufficient in cases involving overlapping or small human poses                                                                                                                                         | Although the method considerably boosts both accuracy and efficiency compared to existing bottom-up approaches, the reliance on ranking of Gaussian responses (and the demonstrated gap between conventional loss functions and the actual localization precision) suggests that further refinements are necessary—especially for extremely crowded scenes                                                                                                                                                                                    |
| Zhe Cao, Gines Hidalgo, Tomas Simon, Shih-En Wei, Yaser Sheikh                                                                                        | OpenPose: Realtime Multi-Person 2D Pose Estimation Using Part Affinity Fields                                              | IEEE Transactions on Pattern Analysis and Machine Intelligence                                      | IEEE                                                                             | 2021                               | 43                      | 1                        | Real-time multi-person 2D human pose detection and association in images and videos                                                                                                                                                                                          | A bottom-up approach that employs a multi-stage CNN with Part Affinity Fields (PAFs) to jointly predict body part locations and the association between parts via a greedy parsing strategy                                                                                                                                                                                                                                                                                                                                                                                        | Publicly released OpenPose library; an annotated foot dataset (~15K instances) is provided along with use of common datasets (e.g., COCO, MPII) for evaluation                                                                                                                                                                   | Claims include approximately 200% speed improvement and a 7% boost in accuracy over previous methods, with competitive performance versus Mask R-CNN and Alpha-Pose                                                                                                                                                                                                                                                                                                                          | Not explicitly mentioned; however, the open-source release suggests scope for further optimization and extension (e.g., to vehicle keypoints)                                                                                                | The greedy matching relaxation may sometimes yield suboptimal associations in extremely crowded scenes; the method relies on high-quality PAF predictions                                                                                                                                                                                                                                                                                             | The method is both innovative and effective for real-time multi-person pose estimation; a more detailed discussion on potential failure cases and robustness under extreme conditions would further strengthen the work                                                                                                                                                                                                                                                                                                                        |
| Leonid Pishchulin, Eldar Insafutdinov, Siyu Tang, Bjoern Andres, Mykhaylo Andriluka, Peter Gehler, Bernt Schiele                                      | DeepCut: Joint Subset Partition and Labeling for Multi-Person Pose Estimation                                              | CVPR (IEEE Conference on Computer Vision and Pattern Recognition)                                   | IEEE                                                                             | 2016                               | N/A                     | N/A                      | Tackles the challenge of estimating the full-body pose of multiple people in real-world images. It addresses issues such as detecting an unknown number of persons, handling occlusions, and disambiguating overlapping body parts                                           | Proposes a joint formulation that simultaneously partitions and labels a set of CNN-generated body part candidates. The solution is cast as an Integer Linear Programming (ILP) problem that integrates unary terms (from CNN-based part detectors) with pairwise terms modeling geometric and appearance constraints                                                                                                                                                                                                                                                              | Uses publicly available datasets – notably Leeds Sports Poses (LSP), LSP Extended (LSPET), and MPII Human Pose – whose data and annotations can be accessed online                                                                                                                                                             | Reports state-of-the-art results: e.g., up to 87.1% PCK on LSP and around 82.4% PCKh on MPII, along with improvements in other metrics (AUC, mPCP, AOP) compared to previous methods                                                                                                                                                                                                                                                                                                         | Future enhancements may include incorporating body part orientation, leveraging multi-resolution context, and moving toward fully end-to-end joint training approaches to further boost performance                                          | The ILP formulation is NP-hard, which means high computational complexity. The approach is also heavily dependent on the initial quality of the CNN-based part detectors and may not scale well in real-time scenarios                                                                                                                                                                                                                                | Although DeepCut significantly outperforms prior methods and advances multi-person pose estimation, its multi-stage training and optimization complexity could limit its practical applicability in time-critical or integrated end-to-end systems                                                                                                                                                                                                                                                                                             |
| Jiaji Liu, Xiaofang Mu, Zhenyu Liu, Hao Li                                                                                                            | Human skeleton behavior recognition model based on multi-object pose estimation with spatiotemporal semantics              | Machine Vision and Applications                                                                     | Springer (implied via DOI)                                                       | 2023                               | 34                      | 44                       | Solves the challenges of multi-object pose estimation in video streams—specifically handling motion blur, pose occlusion, and multi-scale detection issues—to enable accurate human behavior recognition (e.g., classroom behavior identification)                         | An end-to-end framework that first detects multiple objects using an improved YOLOv3 network, then extracts human key points with a lightweight Lite-HRNet model. It integrates temporal cues via a similarity-pose-temporal-merging (SPTM) module and employs a deformable convolution-based pose correction module. Finally, it classifies behavior from the skeleton data using dedicated classifiers                                                                                                                                                                           | The experiments make use of the public PoseTrack2018 dataset for human pose estimation alongside a self-built dataset derived from real classroom videos (captured in 1080p mp4 format via a Hikvision camera). Detailed sample distributions are provided within the paper                                                      | In pose estimation, the proposed model achieved an accuracy of 80.1%—reporting improvements over comparative models (e.g., by 12.5% over AlphaPose and other benchmarks). For behavior classification, machine learning algorithms (e.g., Poly kernel SVM) attained accuracies up to 73.66%, while a deep learning model reached around 70.15% test accuracy                                                                                                                                | The paper does not explicitly outline future work, leaving this area open for further exploration                                                                                                                                            | Limitations are not explicitly discussed; however, while the paper addresses shortcomings in previous methods (such as real-time performance and challenges with small-object images), potential restrictions regarding computational complexity and scalability of the integrated approach might require further clarification                                                                                                                       | The study provides a comprehensive integration of spatiotemporal semantics into multi-object pose estimation and behavior recognition. Although the experimental results are promising, additional analysis regarding the computational efficiency, potential scalability issues, and broader applicability would further strengthen the work                                                                                                                                                                                                  |
| Tong Zhang, Qilin Li, Jingtao Wen, C.L. Philip Chen                                                                                                   | Enhancement and optimisation of human pose estimation with multi-scale spatial attention and adversarial data augmentation | Information Fusion                                                                                  | Elsevier B.V.                                                                    | 2024                               | 111                     | N/A                      | Improves human pose estimation in complex scenes by addressing occlusion, blurring, overfitting, and high model volume challenges                                                                                                                                            | Proposes a novel network that integrates a Multi-Scale Spatial Attention (MSSA) module to refine feature extraction and an Adversarial Data Augmentation (ADA) module to mitigate overfitting. Additionally, self-distillation along with a streamlined Feature Pyramid Network is used to adapt the model for compact devices                                                                                                                                                                                                                                                     | Validated using benchmark datasets MPII and MSCOCO                                                                                                                                                                                                                                                                               | MPII: 92.2% accuracy; MSCOCO: 80.4% accuracy                                                                                                                                                                                                                                                                                                                                                                                                                                                 | Not specified                                                                                                                                                                                                                                | Not explicitly discussed                                                                                                                                                                                                                                                                                                                                                                                                                              | Not provided                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
| Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, C. Lawrence Zitnick                              | Microsoft COCO: Common Objects in Context                                                                                  | ECCV 2014 Proceedings (LNCS 8693)                                                                   | Springer International Publishing Switzerland                                    | 2014                               | 8693                    | N/A                      | It tackles the challenge of object recognition and scene understanding by addressing how to detect non-iconic views, reason about contextual relationships, and precisely localize objects in complex, natural scenes                                                        | A novel pipeline combining data collection with targeted non-iconic image searches and a crowd-sourced annotation process—employing hierarchical category labeling, instance spotting, and detailed instance segmentation—augmented by baseline evaluations (e.g. using Deformable Parts Models)                                                                                                                                                                                                                                                                                 | The MS COCO dataset comprises 328,000 images and 2.5 million labeled object instances spanning 91 categories. Annotations (including instance segmentation and bounding boxes) are publicly released for the train and validation splits (with test annotations held out)                                                        | Baseline evaluations using DPM models show that detection performance on MS COCO (with many occluded and cluttered objects) drops by nearly a factor of 2 compared to PASCAL VOC. Detailed average precision (AP) values per category and AP differences (e.g. 12.7 AP drop when using models trained on PASCAL VOC vs. 7.7 AP drop for models trained on COCO) are reported                                                                                                                 | Future work plans include extending annotations to incorporate "stuff" categories, further refining segmentation quality, and the development of more advanced models to fully exploit the dataset's complexity                              | Challenges include potential annotation noise inherent in crowdsourcing, difficulty in segmenting tightly clustered objects, and the inherent challenge of dealing with non-iconic, cluttered images                                                                                                                                                                                                                                                  | The baseline models (such as traditional DPM approaches) demonstrate significant performance drops on this challenging dataset, highlighting that while the dataset is rich and comprehensive, fully leveraging its complexity will require more sophisticated, state-of-the-art algorithms                                                                                                                                                                                                                                                    |
| Eric Zimmermann, Justin Szeto, Jerome Pasquero, Frederic Ratle                                                                                        | Benchmarking a Benchmark: How Reliable is MS-COCO?                                                                         | arXiv Preprint (cs.CV)                                                                              | N/A (Preprint available on arXiv)                                                | 2023                               | N/A                     | N/A                      | Investigates the reliability and quality of the MS-COCO dataset by uncovering annotation biases, discrepancies, and the impact of annotation style on model performance in computer vision tasks                                                                             | Introduces a re-annotation (Sama‑COCO) of MS‑COCO and applies a shape analysis pipeline that uses an IoU-based matching strategy. A Faster R‑CNN (with a ResNet‑50 and FPN backbone) is trained and evaluated on both the original and re-annotated datasets using the Detectron2 framework                                                                                                                                                                                                                                                                                    | Builds on the original MS‑COCO. The newly re-annotated Sama‑COCO dataset is made publicly available at https://www.sama.com/sama-coco-dataset/                                                                                                                                                                                 | Evaluated using mean Average Precision (mAP) on detection and instance segmentation tasks. For example, Table 1 provides detection mAP values of ≈40.26 for MS‑COCO versus ≈39.09 for Sama‑COCO, along with additional metrics such as mAP@50, mAPLarge, mAPMedium, and mAPSmall. Detailed quantitative comparisons are presented in Tables 1 & 2                                                                                                                                        | While no explicit "future works" section is provided, the discussion hints at the need to further explore labeling bias, refine annotation guidelines, and investigate how annotation styles influence downstream neural network performance | Despite the improvements in polygon tightness and segmentation, the re-annotation does not completely eliminate inherent dataset biases. Also, merging datasets with conflicting annotation styles may lead to unpredictable model behavior                                                                                                                                                                                                           | One potential critique is that the study's evaluation is limited to a single model architecture (Faster R‑CNN) and does not offer broad guidelines for systematically improving annotation protocols across varied settings                                                                                                                                                                                                                                                                                                                   |
| Tairan He, Wenli Xiao, Toru Lin, Zhengyi Luo, Zhenjia Xu, Zhenyu Jiang, Jan Kautz, Changliu Liu, Guanya Shi, Xiaolong Wang, Linxi "Jim" Fan, Yuke Zhu | HOVER: Versatile Neural Whole-Body Controller for Humanoid Robots                                                          | arXiv (preprint)                                                                                    | N/A                                                                              | 2024                               | N/A                     | N/A                      | Overcomes the limitations of task‐ or mode‐specific controllers by unifying diverse whole‐body control tasks (e.g., locomotion, manipulation, navigation) into a seamless framework                                                                                       | A multi‐mode policy distillation framework that: (1) Trains an oracle controller using full-body kinematic motion imitation from large-scale human motion capture data via reinforcement learning (PPO) and supervised learning (using the DAgger framework), (2) Employs mode-specific and sparsity-based command masking to enable a unified policy for diverse control modes                                                                                                                                                                                                   | Uses large-scale human motion capture datasets (e.g., AMASS/MoCap) that are retargeted to the humanoid platform (termed "dataset ˆQ"). Although specifics are not provided, these datasets are typically publicly available                                                                                                     | Evaluated with metrics such as upper/lower joint errors (rad), global/local body position errors (mm), root velocity (m/s), and root rotation (rad). For example: In simulation: HOVER achieved an Eg-mpjpe of ~63.9 mm (compared with much higher error values for some specialist controllers). Real-world tests (ExBody mode) showed Eg-mpjpe of ~48.9 mm                                                                                                                                 | Explore development of an automated mode-switching module for real-world applications                                                                                                                                                        | The approach is demonstrated on a specific 19-DOF humanoid (UnitreeH1) and its effectiveness depends on the quality and coverage of the motion capture data used during oracle training; the method may face challenges under highly dynamic or unforeseen conditions                                                                                                                                                                                 | While the unified controller shows promising versatility and lower tracking errors than specialist policies, its reliance on imitation from an oracle policy may restrict adaptability to novel scenarios. Further evaluations across diverse hardware and extreme dynamic tasks are needed to fully assess its robustness                                                                                                                                                                                                                     |
| Veenu Rani, Munish Kumar                                                                                                                              | Transfer learning for human gait recognition using VGG19: CASIA-A dataset                                                  | Multimedia Tools and Applications                                                                   | Springer Science+Business Media, LLC (part of Springer Nature)                   | 2024                               | Not explicitly provided | Not specified            | Identification of individuals via their walking patterns (gait recognition) as a biometric modality                                                                                                                                                                          | A transfer learning framework based on the VGG19 deep learning model. The technique involves using VGG19 for feature extraction combined with a Softmax classifier and tuning key hyperparameters                                                                                                                                                                                                                                                                                                                                                                                  | Uses the public CASIA-A benchmark dataset comprising 19,139 images captured from 20 individuals. The dataset is segmented with two schemes (70:30 and 80:20) and is publicly available for further research                                                                                                                      | For the 70:30 scheme at a learning rate (LR) of 0.06: Accuracy = 96.9% with a loss of 2.71%; For the 80:20 scheme at LR = 0.09: Accuracy = 97.8% with a loss of 2.01%                                                                                                                                                                                                                                                                                                                        | The authors plan to develop their own corpus to further test and validate the proposed methodology                                                                                                                                           | Not explicitly detailed; the study is limited to experiments on the CASIA-A dataset which may restrict insights regarding generalizability across different environments or conditions                                                                                                                                                                                                                                                                | While the method demonstrates high accuracy, further discussion on computational cost, robustness across varied real-world conditions, and validation using additional, diverse datasets would strengthen the work                                                                                                                                                                                                                                                                                                                             |
| Linfang Yu, Zhen Qin*, Liqun Xu, Zhiguang Qin, Kim-Kwang Raymond Choo                                                                                 | SSpose: Self-supervised Spatial-aware Model for Human Pose Estimation                                                      | IEEE Transactions on Artificial Intelligence                                                        | IEEE                                                                             | 2024                               | N/A                     | N/A                      | Addresses challenges in human pose estimation—specifically, capturing global spatial dependencies between body parts and enhancing model interpretability in complex scenarios (e.g., occlusions and interactions)                                                          | A hybrid CNN-Transformer model named SSpose. The model uses a multi-scale fusion module to merge coarse- and fine-grained image features, employs the final attention layer of the Transformer for keypoint localization, and integrates a self‑supervised training framework based on masked convolution and a hierarchical masking strategy (leveraging the MAE concept)                                                                                                                                                                                                        | Evaluated on public datasets: COCO validation set, COCO test‑dev set, and MPII; code is available on GitHub (https://github.com/yulinfangylf/SSpose)                                                                                                                                                                            | On the COCO val set, AP = 77.3% and AR = 82.1%; on the COCO test‑dev set, AP = 76.4% and AR = 81.5%; demonstrated strong generalization on MPII                                                                                                                                                                                                                                                                                                                                             | Not explicitly mentioned in the available text                                                                                                                                                                                               | The article discusses challenges such as visible information leakage inherent in convolutional reconstruction and the need for a carefully designed hierarchical masking strategy, though explicit limitations for SSpose are not exhaustively detailed                                                                                                                                                                                               | The document does not include an explicit critique; however, potential points for discussion might include the complexity of integrating convolutional and transformer components and the possible high computational resources required                                                                                                                                                                                                                                                                                                       |
| Alexander Toshev, Christian Szegedy                                                                                                                   | DeepPose: Human Pose Estimation via Deep Neural Networks                                                                   | arXiv                                                                                               | Google (address: 1600 Amphitheatre Pkwy, Mountain View, CA)                      | 2014                               | v3                      | N/A                      | Precise human pose estimation by localizing body joints even under extreme articulations, occlusions, and scale variations                                                                                                                                                   | Formulated the task as a regression problem solved with a cascade of deep neural network regressors – a 7-layer convolutional network predicts joint coordinates from the full image, then subsequent stages refine predictions using cropped sub-images                                                                                                                                                                                                                                                                                                                          | Uses publicly available academic benchmarks (e.g., FLIC, LSP, Image Parse) augmented with random translations, flips, etc                                                                                                                                                                                                        | Evaluated by metrics such as Percentage of Correct Parts (PCP) and Percent of Detected Joints (PDJ). For instance, DeepPose-st1 shows PCP values of around 0.50 for arms and 0.27 for legs, with later stages further improving precision                                                                                                                                                                                                                                                    | Explore novel architectures tailored more closely to localization challenges and refine cascade strategies                                                                                                                                   | Some joint localization challenges remain (e.g., occasional left/right confusion and resolution loss due to pooling layers) and training is computationally intensive                                                                                                                                                                                                                                                                                 | While achieving state-of-art performance, the use of a generic CNN (originally designed for classification) may not be optimal for high-precision joint localization; future work should consider more domain-specific modifications                                                                                                                                                                                                                                                                                                           |
| Daniil Osokin                                                                                                                                         | Real-time 2D Multi-Person Pose Estimation on CPU: Lightweight OpenPose                                                     | arXiv                                                                                               | arXiv                                                                            | 2018                               | N/A                     | N/A                      | Achieve real‑time multi‑person pose estimation on edge devices (CPUs)                                                                                                                                                                                                      | An optimized bottom‑up approach derived from OpenPose, incorporating a dilated MobileNet v1 backbone with depthwise separable convolutions, a shared prediction branch in the refinement stage, and fast post‑processing optimizations                                                                                                                                                                                                                                                                                                                                           | Trained on the COCO dataset; code and model are available as part of the Intel® OpenVINO™ Toolkit                                                                                                                                                                                                                              | Inference speeds of 28 fps on an Intel NUC6i7KYB mini PC and 26 fps on a Core i7‑6850K CPU; ~4.1M parameters; 9 GFLOPs; AP drop of less than 1% relative to the baseline                                                                                                                                                                                                                                                                                                                    | Investigating quantization, pruning, and knowledge distillation                                                                                                                                                                              | Reliance on upsampling for maintaining accuracy; evaluation primarily on the COCO dataset                                                                                                                                                                                                                                                                                                                                                             | Lacks extensive comparative analysis with a broader set of state‑of‑the‑art methods                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
| Jun Liu, Amir Shahroudy, Mauricio Perez, Gang Wang, Ling-Yu Duan, Alex C. Kot                                                                         | NTU RGB+D120: A Large-Scale Benchmark for 3D Human Activity Understanding                                                  | arXiv                                                                                               | N/A                                                                              | 2019                               | N/A                     | N/A                      | Provides a large‐scale, diverse dataset for 3D human activity recognition and introduces a novel one-shot recognition problem for action classification                                                                                                                     | Uses multi-modal data (RGB, depth, skeleton, infrared) captured via Kinect v2; proposes an Action-Part Semantic Relevance-aware (APSR) framework built on a bidirectional ST-LSTM–inspired network to emphasize body parts tied to semantic descriptions                                                                                                                                                                                                                                                                                                                          | NTU RGB+D120 dataset consists of 114,480 video samples across 120 action classes, recorded from 106 subjects with 155 camera views; available at http://rose1.ntu.edu.sg/Datasets/actionRecognition.asp                                                                                                                          | Evaluation using cross‐subject and cross‐setup protocols (e.g., ST-LSTM model achieving ~77.7% accuracy on cross‐subject and ~57.9% on cross‐setup evaluations; similar figures reported for an MTLN method)                                                                                                                                                                                                                                                                             | The paper hints at expanding the research in one-shot recognition, suggesting further refinement of the APSR framework and the exploration of additional deep learning architectures for improved performance                                | One inherent challenge is the difficulty of generalizing from one learning example in one-shot recognition; accurately estimating semantic relevance for fine-grained actions remains challenging                                                                                                                                                                                                                                                     | The work is innovative in providing a comprehensive benchmark and a new framework; however, more extensive evaluations under varied real-world conditions and deeper analysis of issues like sample bias or robustness under noisy conditions could further strengthen the study                                                                                                                                                                                                                                                               |
| Zhe Zhang, Jie Tang, Gangshan Wu                                                                                                                      | Simple and Lightweight Human Pose Estimation                                                                               | arXiv preprint                                                                                      | arXiv                                                                            | 2020                               | N/A                     | N/A                      | Enable efficient human pose estimation on resource‐limited devices by reducing model size and computational cost while preserving competitive accuracy                                                                                                                      | Redesigned the standard bottleneck block by incorporating depthwise convolution and a global context (attention) mechanism; proposed the Lightweight Pose Network (LPN) built on these blocks; introduced an iterative training strategy and a model-agnostic post-processing method (Soft-Argmax)                                                                                                                                                                                                                                                                                 | Evaluated on the publicly available COCO keypoint detection dataset (including train2017, val2017, and test-dev2017) with detailed data augmentation (e.g., rotation, scaling, flipping)                                                                                                                                         | For instance, the LPN-50 achieves roughly 68.7–69.1 AP with only 2.7–2.9M parameters, 1.0 GFLOPs, and an inference speed of 17 FPS on an Intel i7-8700K CPU. Comprehensive analyses and ablation studies are provided                                                                                                                                                                                                                                                                      | The authors express hope that their work will inspire further developments in lightweight human pose estimation and improved training strategies                                                                                             | A slight drop in accuracy compared to more complex models (e.g. a 1.3 AP gap relative to SimpleBaseline) and challenges in training small networks (e.g. tendency to fall into local minima)                                                                                                                                                                                                                                                          | While the approach offers significant efficiency benefits, the improvement yielded by the Soft-Argmax component is modest, and the iterative training strategy—though effective—adds complexity to the training process                                                                                                                                                                                                                                                                                                                      |
| Sohyun Lee; Jaesung Rim; Haechan Lee; Boseung Jeong; Sunghyun Cho; Geonu Kim; Suha Kwak; Byungju Woo                                                  | Human Pose Estimation in Extremely Low-Light Conditions                                                                    | N/A                                                                                                 | N/A                                                                              | Not specified                      | N/A                     | N/A                      | Estimation of human poses from single sRGB images captured in extremely low-light conditions, overcoming challenges related to poor visibility and difficulties in acquiring accurate pose labels                                                                            | A novel approach that (1) employs a dedicated dual‐camera system to collect paired low‐light and well‐lit images, (2) introduces the ExLPose dataset, (3) adopts a teacher–student framework based on Learning Using Privileged Information (LUPI), and (4) integrates Lighting-Specific Batch Normalization (LSBN) layers to compensate for differences in lighting                                                                                                                                                                                                           | ExLPose dataset containing 2,556 paired images (2,065 for training and 491 for testing) with 14,215 human pose annotations; additionally, the ExLPose-OCN dataset offers extremely low-light images captured by alternative cameras. Data were collected using a custom dual-camera system ensuring real and aligned image pairs | Evaluated using the AP@0.5:0.95 metric: on ExLPose, the proposed method ("Ours") achieved scores of 42.3 (LL-N), 34.0 (LL-H), 18.6 (LL-E), 32.7 (LL-A), and 68.5 (WL), with an average AP ≈35.2 on the ExLPose-OCN dataset                                                                                                                                                                                                                                                                  | Not explicitly stated; however, potential directions include extending the approach to cover diverse camera systems and natural low-light scenarios beyond the ND-filter setting                                                             | The dataset is limited to a specific camera system (thus not covering a wide variety of cameras), and the low-light images are acquired using an ND filter—which may not fully represent typical night-time low-light conditions                                                                                                                                                                                                                     | The method relies on paired well-lit images as privileged information during training, which might restrict its applicability in scenarios where such paired data are hard to obtain; broader generalization to other types of low-light conditions remains to be validated                                                                                                                                                                                                                                                                    |
| Soonchan Park, Jinah Park                                                                                                                             | Localizing Human Keypoints beyond the Bounding Box                                                                         | ICCVW 2021*                                                                                         | Not explicitly stated                                                            | 2021                               | N/A                     | N/A                      | Overcome the detector's failure to localize keypoints outside the bounding box in partial images                                                                                                                                                                             | Introduces a two-part approach: (1) Position Puzzle Network (PPNet) that refines the bounding box to enlarge the area for keypoint estimation, and (2) Position Puzzle Augmentation, a training augmentation strategy that uses systematically cropped parts of the image with complete keypoint labels                                                                                                                                                                                                                                                                            | Uses the publicly available COCO dataset plus a derived Crop-COCO dataset (through systematic cropping) to simulate partial images with full keypoint annotations                                                                                                                                                                | On Crop-COCO, the proposed method improves baseline performance by up to 37.6% (mAP) and 30.6% (mAR); on the original COCO dataset, improvements are more modest (approximately 0.6–0.9% increase in mAP/mAR). Detailed results are reported in multiple tables within the paper                                                                                                                                                                                                            | Not explicitly mentioned                                                                                                                                                                                                                     | The method is sensitive to the "puzzle" (crop) size – if too small, the extracted features may be insufficient. Moreover, when key regions (e.g., ankles in an image showing only the head and shoulders) lack visual cues, the method may fail to accurately localize some keypoints                                                                                                                                                                | While the approach clearly enhances keypoint estimation for partial images, it yields only marginal gains for whole-image scenarios. Furthermore, the reliance on contrived cropping policies may not fully mimic real-world occlusion or partial visibility, which might limit generalizability                                                                                                                                                                                                                                               |
| Luigi D'Alfonso, Emanuele Garone, Pietro Muraca, Paolo Pugliese                                                                                       | Camera and inertial sensor fusion for the PnP problem: algorithms and experimental results                                 | Machine Vision and Applications                                                                     | Springer (inferred via DOI and journal styling)                                  | 2021                               | 32                      | 90                       | Estimation of the relative pose (position and orientation) between a camera and an object from image and inertial sensor data (solving the PnP problem)                                                                                                                      | A fusion approach using only the accelerometer (inclinometer) measurements from IMUs to complement camera images. The paper develops two main algorithmic frameworks: (1) for the P2P case (with ambiguous dual solutions and a heuristic discard procedure) and (2) an extension to P3P and the general PnP problem by formulating and minimizing a least-squares reprojection error (with translations computed via closed‐form pseudo‑inverse solutions and refinements using optimization techniques such as Nelder–Mead)                                                   | Simulated data (using randomly generated feature configurations corrupted by Gaussian noise) and real experimental tests (using a Logitech HD camera paired with ArduIMUv3 sensors). The paper does not explicitly state whether the data is publicly available                                                                  | The performance is evaluated in terms of:`<br>`- Rotation error: measured in degrees, with comparisons against eight competing methods. `<br>`- Translation error: e.g., an average of 0.029 m (with the pseudo‑inverse approach) versus 0.24 m by a direct method. `<br>`- Computational speed: CPU time improvements—with the proposed method being 20 to 100 times faster (down to approximately 0.002 s in some cases vs. 0.556 s for numerical optimization in competitors) | Future work is not explicitly detailed; however, the discussion suggests potential extensions could include a further reduction of pixel noise effects and possibly the integration of additional sensor modalities                          | Ambiguities occur in degenerate configurations such as:`<br>`- When landmarks are collinear or when two pixel projections coincide (yielding infinite or ambiguous solutions). `<br>`- The direct translation computation (via Eq. 17) is sensitive to pixel noise, which is mitigated only by a subsequent least-squares minimization step. `<br>`- The method deliberately neglects the magnetometer data from the IMUs due to its inaccuracy | Although the proposed approach achieves excellent computational efficiency and balanced accuracy (rotation and translation) in controlled simulations and experiments, its heavy reliance on accelerometer-only fusion and ideal configurations raises questions about its robustness under more unconstrained or adverse real-world conditions. Additionally, while the method is compared favorably against several competitive algorithms, further validation on public or more diverse datasets would strengthen its general applicability |
| Shuxi Wang; Jiahui Pan; Binyuan Huang; Pingzhi Liu; Zina Li; Chengju Zhou                                                                             | ICE‑GCN: An interactional channel excitation‑enhanced graph convolutional network for skeleton‑based action recognition | Machine Vision and Applications                                                                     | Springer (inferred from the DOI)                                                 | 2023                               | 34                      | Not specified            | Improves skeleton‑based action recognition by addressing the limitation of treating all spatial and temporal features equally and missing cross‑dimensional interactions                                                                                                   | Introduces the ICE module composed of two sub‑modules – channel‑wise spatial excitation (CSE) and channel‑wise temporal excitation (CTE) – integrated into a graph convolutional network. Additionally, a complementary topology scheme is proposed combining three adjacency matrices (predefined, learnable, and similarity‑based) to enhance feature representation                                                                                                                                                                                                       | Evaluated on established public datasets: NTU RGB+D 60, NTU RGB+D 120, and Kinetics‑Skeleton. The code is planned to be published on GitHub (https://github.com/shuxiwang/ICE-GCN)                                                                                                                                              | On NTU RGB+D 60 (Cross-View setting), improvement is shown from a baseline AGCN accuracy of 93.7% to 94.8% (a 1.1% boost). Also noted is a modest increase in computation (approximately +0.03 GFLOPs and +0.42M parameters)                                                                                                                                                                                                                                                                 | Not explicitly provided                                                                                                                                                                                                                      | Not explicitly mentioned                                                                                                                                                                                                                                                                                                                                                                                                                              | No direct critique is offered in the paper                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
